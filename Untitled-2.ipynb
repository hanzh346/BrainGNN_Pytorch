{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "hang_FDG = pd.read_csv('/media/hang/EXTERNAL_US/Data/1_HANG_FDG_PET/ADNI_Second_organized/1_HANG_FDG_PET_1_30_2024.csv')\n",
    "hang_FDG[hang_FDG['Description'].str.contains('6mm')].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ADNI_merge = pd.read_csv('/media/hang/EXTERNAL_US/Data/1_HANG_FDG_PET/ADNIMERGE_30Jan2024.csv')\n",
    "ADNI_merge[ADNI_merge['AV45_bl'] < 0.79]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hang_FDG_all = pd.read_csv('/media/hang/EXTERNAL_US/Data/1_HANG_FDG_PET/longitudinal_AD_MCI_CN/4_TRIAL_HANG_5_10_2024.csv')\n",
    "hang_FDG_all[hang_FDG_all['Description'].str.contains('6mm')].shape\n",
    "\n",
    "hang_FDG_ab_pos = pd.read_csv('/media/hang/EXTERNAL_US/Data/1_HANG_FDG_PET/longitudinal_AD_MCI_CN/3_HANG_LONG_5_03_2024.csv')\n",
    "\n",
    "ab_pos_cases = hang_FDG_ab_pos[hang_FDG_ab_pos['Description'].str.contains('6mm')]['Subject'].drop_duplicates()\n",
    "print(len(set(ab_pos_cases)))\n",
    "\n",
    "hang_FDG_CN_ab_neg = pd.read_csv('/media/hang/EXTERNAL_US/Data/1_HANG_FDG_PET/longitudinal_CN/4_ALL_FDG_3_27_2024.csv')\n",
    "ab_neg_cases = hang_FDG_CN_ab_neg[hang_FDG_CN_ab_neg['Description'].str.contains('6mm')]['Subject'].drop_duplicates()\n",
    "print(len(set(ab_neg_cases)))\n",
    "\n",
    "\n",
    "union_set_longitudinal = set(ab_pos_cases).union(set(ab_neg_cases))\n",
    "union_set_longitudinal\n",
    "intersect_set_longitudinal = set(ab_pos_cases).intersection(set(ab_neg_cases))\n",
    "intersect_set_longitudinal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hang_FDG_ab_pos_MRI = hang_FDG_ab_pos[hang_FDG_ab_pos['Description'].str.contains('MT1')]\n",
    "print(pd.DataFrame(hang_FDG_ab_pos_MRI).sort_values(by='Subject').drop_duplicates(subset='Subject',keep='first'))\n",
    "hang_FDG_ab_pos_PET = hang_FDG_ab_pos[hang_FDG_ab_pos['Description'].str.contains('6mm')]\n",
    "#print(pd.DataFrame(hang_FDG_ab_pos_PET).sort_values(by='Subject').drop_duplicates(subset='Subject',keep='first'))\n",
    "\n",
    "hang_FDG_ab_pos_PET['Acq Date'] = pd.to_datetime(hang_FDG_ab_pos_PET['Acq Date'])\n",
    "hang_FDG_ab_pos_MRI['Acq Date'] = pd.to_datetime(hang_FDG_ab_pos_MRI['Acq Date'])\n",
    "print('overlaped_ab+ PET and MRI',len(set(hang_FDG_ab_pos_PET['Subject']) & set(hang_FDG_ab_pos_MRI['Subject'])))\n",
    "\n",
    "hang_FDG_ab_neg_MRI = hang_FDG_CN_ab_neg[hang_FDG_CN_ab_neg['Description'].str.contains('MT1')]\n",
    "hang_FDG_ab_neg_PET = hang_FDG_CN_ab_neg[hang_FDG_CN_ab_neg['Description'].str.contains('6mm')]\n",
    "\n",
    "hang_FDG_ab_neg_PET['Acq Date'] = pd.to_datetime(hang_FDG_ab_neg_PET['Acq Date'])\n",
    "hang_FDG_ab_neg_MRI['Acq Date'] = pd.to_datetime(hang_FDG_ab_neg_MRI['Acq Date'])\n",
    "print('overlaped_ab- PET and MRI',len(set(hang_FDG_ab_neg_PET['Subject']) & set(hang_FDG_ab_neg_MRI['Subject'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Initialize an empty list to store results\n",
    "smallest_gaps = []\n",
    "\n",
    "# Process each subject's data separately\n",
    "for subject in hang_FDG_ab_pos_PET['Subject'].unique():\n",
    "    # Get all PET and MRI records for this subject\n",
    "    pet_subject = hang_FDG_ab_pos_PET[hang_FDG_ab_pos_PET['Subject'] == subject]\n",
    "    mri_subject = hang_FDG_ab_pos_MRI[hang_FDG_ab_pos_MRI['Subject'] == subject]\n",
    "   \n",
    "\n",
    "    # Initialize the minimum gap as a large value\n",
    "    min_gap = pd.Timedelta(days=9999)\n",
    "\n",
    "    # Compare each PET date with every MRI date\n",
    "    if mri_subject.empty:\n",
    "        print('No MRI subjects for {}'.format(subject))\n",
    "    for pet_date in pet_subject['Acq Date']:\n",
    "        for mri_date in mri_subject['Acq Date']:\n",
    "            # Calculate the absolute difference\n",
    "            gap = abs(pet_date - mri_date)\n",
    "            # Update the minimum gap if the current one is smaller\n",
    "            if gap < min_gap:\n",
    "                min_gap = gap\n",
    "                best_pair = (pet_date, mri_date)\n",
    "\n",
    "    # If there is a valid match, add the smallest gap information\n",
    "    if min_gap != pd.Timedelta(days=9999):\n",
    "        if min_gap > pd.Timedelta(days=365):\n",
    "            continue\n",
    "        smallest_gaps.append({\n",
    "            'Subject': subject,\n",
    "            'PET_Date': best_pair[0],\n",
    "            'MRI_Date': best_pair[1],\n",
    "            'Smallest_Gap': min_gap\n",
    "        })\n",
    "\n",
    "# Convert the results to a DataFrame\n",
    "smallest_gaps_df = pd.DataFrame(smallest_gaps).drop_duplicates()\n",
    "#smallest_gaps_df.to_csv('/media/hang/EXTERNAL_US/Data/1_HANG_FDG_PET/longitudinal_CN/negative_longitudinal_PET_MRI_pair.csv')\n",
    "smallest_gaps_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>ImageDataID</th>\n",
       "      <th>Subject</th>\n",
       "      <th>Group</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>Visit</th>\n",
       "      <th>Modality</th>\n",
       "      <th>Description</th>\n",
       "      <th>Type</th>\n",
       "      <th>...</th>\n",
       "      <th>RIGHT_VENTRALDC_SUVR</th>\n",
       "      <th>RIGHT_VENTRALDC_VOLUME</th>\n",
       "      <th>RIGHT_VESSEL_SUVR</th>\n",
       "      <th>RIGHT_VESSEL_VOLUME</th>\n",
       "      <th>WM_HYPOINTENSITIES_SUVR</th>\n",
       "      <th>WM_HYPOINTENSITIES_VOLUME</th>\n",
       "      <th>TOTAL_INTRACRANIAL_VOLUME</th>\n",
       "      <th>update_stamp_y</th>\n",
       "      <th>binary_labels_cross_section</th>\n",
       "      <th>binary_labels_longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>I1593083</td>\n",
       "      <td>002_S_4171</td>\n",
       "      <td>LMCI</td>\n",
       "      <td>M</td>\n",
       "      <td>69</td>\n",
       "      <td>v03</td>\n",
       "      <td>PET</td>\n",
       "      <td>Coreg, Avg, Std Img and Vox Siz, Uniform 6mm Res</td>\n",
       "      <td>Processed</td>\n",
       "      <td>...</td>\n",
       "      <td>1.6535</td>\n",
       "      <td>4032</td>\n",
       "      <td>1.6759</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.8061</td>\n",
       "      <td>783</td>\n",
       "      <td>1.576281e+06</td>\n",
       "      <td>2021-01-19 10:55:04</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>I1593114</td>\n",
       "      <td>002_S_4213</td>\n",
       "      <td>CN</td>\n",
       "      <td>F</td>\n",
       "      <td>78</td>\n",
       "      <td>v03</td>\n",
       "      <td>PET</td>\n",
       "      <td>Coreg, Avg, Std Img and Vox Siz, Uniform 6mm Res</td>\n",
       "      <td>Processed</td>\n",
       "      <td>...</td>\n",
       "      <td>1.3132</td>\n",
       "      <td>3689</td>\n",
       "      <td>1.1143</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1.4608</td>\n",
       "      <td>1476</td>\n",
       "      <td>1.508126e+06</td>\n",
       "      <td>2021-01-19 10:55:04</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>I1593109</td>\n",
       "      <td>002_S_4225</td>\n",
       "      <td>CN</td>\n",
       "      <td>M</td>\n",
       "      <td>70</td>\n",
       "      <td>v03</td>\n",
       "      <td>PET</td>\n",
       "      <td>Coreg, Avg, Std Img and Vox Siz, Uniform 6mm Res</td>\n",
       "      <td>Processed</td>\n",
       "      <td>...</td>\n",
       "      <td>1.6963</td>\n",
       "      <td>4545</td>\n",
       "      <td>1.6395</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.7398</td>\n",
       "      <td>2185</td>\n",
       "      <td>1.908584e+06</td>\n",
       "      <td>2021-01-19 10:55:04</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>I1593115</td>\n",
       "      <td>002_S_4262</td>\n",
       "      <td>CN</td>\n",
       "      <td>F</td>\n",
       "      <td>73</td>\n",
       "      <td>v03</td>\n",
       "      <td>PET</td>\n",
       "      <td>Coreg, Avg, Std Img and Vox Siz, Uniform 6mm Res</td>\n",
       "      <td>Processed</td>\n",
       "      <td>...</td>\n",
       "      <td>1.6008</td>\n",
       "      <td>3464</td>\n",
       "      <td>1.6747</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.6857</td>\n",
       "      <td>6717</td>\n",
       "      <td>1.552242e+06</td>\n",
       "      <td>2021-01-19 10:55:04</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>I1593110</td>\n",
       "      <td>002_S_4270</td>\n",
       "      <td>CN</td>\n",
       "      <td>F</td>\n",
       "      <td>75</td>\n",
       "      <td>v03</td>\n",
       "      <td>PET</td>\n",
       "      <td>Coreg, Avg, Std Img and Vox Siz, Uniform 6mm Res</td>\n",
       "      <td>Processed</td>\n",
       "      <td>...</td>\n",
       "      <td>1.7111</td>\n",
       "      <td>3514</td>\n",
       "      <td>1.5387</td>\n",
       "      <td>34.0</td>\n",
       "      <td>1.7918</td>\n",
       "      <td>2820</td>\n",
       "      <td>1.495694e+06</td>\n",
       "      <td>2021-01-19 10:55:04</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1337</th>\n",
       "      <td>1347</td>\n",
       "      <td>I1603417</td>\n",
       "      <td>941_S_4377</td>\n",
       "      <td>LMCI</td>\n",
       "      <td>F</td>\n",
       "      <td>70</td>\n",
       "      <td>v03</td>\n",
       "      <td>PET</td>\n",
       "      <td>Coreg, Avg, Std Img and Vox Siz, Uniform 6mm Res</td>\n",
       "      <td>Processed</td>\n",
       "      <td>...</td>\n",
       "      <td>1.3244</td>\n",
       "      <td>3307</td>\n",
       "      <td>1.0539</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.5795</td>\n",
       "      <td>972</td>\n",
       "      <td>1.311141e+06</td>\n",
       "      <td>2021-01-19 10:55:04</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1339</th>\n",
       "      <td>1349</td>\n",
       "      <td>I1603375</td>\n",
       "      <td>941_S_4420</td>\n",
       "      <td>EMCI</td>\n",
       "      <td>M</td>\n",
       "      <td>82</td>\n",
       "      <td>v03</td>\n",
       "      <td>PET</td>\n",
       "      <td>Coreg, Avg, Std Img and Vox Siz, Uniform 6mm Res</td>\n",
       "      <td>Processed</td>\n",
       "      <td>...</td>\n",
       "      <td>1.2943</td>\n",
       "      <td>3942</td>\n",
       "      <td>1.3754</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.3988</td>\n",
       "      <td>3596</td>\n",
       "      <td>1.664610e+06</td>\n",
       "      <td>2021-01-19 10:55:04</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1341</th>\n",
       "      <td>1351</td>\n",
       "      <td>I1603380</td>\n",
       "      <td>941_S_4764</td>\n",
       "      <td>EMCI</td>\n",
       "      <td>F</td>\n",
       "      <td>83</td>\n",
       "      <td>v03</td>\n",
       "      <td>PET</td>\n",
       "      <td>Coreg, Avg, Std Img and Vox Siz, Uniform 6mm Res</td>\n",
       "      <td>Processed</td>\n",
       "      <td>...</td>\n",
       "      <td>1.2174</td>\n",
       "      <td>3725</td>\n",
       "      <td>1.0669</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1.2939</td>\n",
       "      <td>8413</td>\n",
       "      <td>1.488525e+06</td>\n",
       "      <td>2021-01-19 10:55:05</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1343</th>\n",
       "      <td>1353</td>\n",
       "      <td>I1603596</td>\n",
       "      <td>941_S_5124</td>\n",
       "      <td>SMC</td>\n",
       "      <td>F</td>\n",
       "      <td>77</td>\n",
       "      <td>v03</td>\n",
       "      <td>PET</td>\n",
       "      <td>Coreg, Avg, Std Img and Vox Siz, Uniform 6mm Res</td>\n",
       "      <td>Processed</td>\n",
       "      <td>...</td>\n",
       "      <td>1.6743</td>\n",
       "      <td>2620</td>\n",
       "      <td>1.4851</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.2004</td>\n",
       "      <td>6689</td>\n",
       "      <td>1.494192e+06</td>\n",
       "      <td>2021-01-19 10:55:05</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1345</th>\n",
       "      <td>1355</td>\n",
       "      <td>I1603407</td>\n",
       "      <td>941_S_5193</td>\n",
       "      <td>SMC</td>\n",
       "      <td>F</td>\n",
       "      <td>73</td>\n",
       "      <td>v03</td>\n",
       "      <td>PET</td>\n",
       "      <td>Coreg, Avg, Std Img and Vox Siz, Uniform 6mm Res</td>\n",
       "      <td>Processed</td>\n",
       "      <td>...</td>\n",
       "      <td>1.2684</td>\n",
       "      <td>3865</td>\n",
       "      <td>0.8988</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.3668</td>\n",
       "      <td>2491</td>\n",
       "      <td>1.468981e+06</td>\n",
       "      <td>2021-01-19 10:55:06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>531 rows × 377 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0 ImageDataID     Subject Group Sex  Age Visit Modality  \\\n",
       "1              1    I1593083  002_S_4171  LMCI   M   69   v03      PET   \n",
       "3              3    I1593114  002_S_4213    CN   F   78   v03      PET   \n",
       "7              7    I1593109  002_S_4225    CN   M   70   v03      PET   \n",
       "15            15    I1593115  002_S_4262    CN   F   73   v03      PET   \n",
       "17            17    I1593110  002_S_4270    CN   F   75   v03      PET   \n",
       "...          ...         ...         ...   ...  ..  ...   ...      ...   \n",
       "1337        1347    I1603417  941_S_4377  LMCI   F   70   v03      PET   \n",
       "1339        1349    I1603375  941_S_4420  EMCI   M   82   v03      PET   \n",
       "1341        1351    I1603380  941_S_4764  EMCI   F   83   v03      PET   \n",
       "1343        1353    I1603596  941_S_5124   SMC   F   77   v03      PET   \n",
       "1345        1355    I1603407  941_S_5193   SMC   F   73   v03      PET   \n",
       "\n",
       "                                           Description       Type  ...  \\\n",
       "1     Coreg, Avg, Std Img and Vox Siz, Uniform 6mm Res  Processed  ...   \n",
       "3     Coreg, Avg, Std Img and Vox Siz, Uniform 6mm Res  Processed  ...   \n",
       "7     Coreg, Avg, Std Img and Vox Siz, Uniform 6mm Res  Processed  ...   \n",
       "15    Coreg, Avg, Std Img and Vox Siz, Uniform 6mm Res  Processed  ...   \n",
       "17    Coreg, Avg, Std Img and Vox Siz, Uniform 6mm Res  Processed  ...   \n",
       "...                                                ...        ...  ...   \n",
       "1337  Coreg, Avg, Std Img and Vox Siz, Uniform 6mm Res  Processed  ...   \n",
       "1339  Coreg, Avg, Std Img and Vox Siz, Uniform 6mm Res  Processed  ...   \n",
       "1341  Coreg, Avg, Std Img and Vox Siz, Uniform 6mm Res  Processed  ...   \n",
       "1343  Coreg, Avg, Std Img and Vox Siz, Uniform 6mm Res  Processed  ...   \n",
       "1345  Coreg, Avg, Std Img and Vox Siz, Uniform 6mm Res  Processed  ...   \n",
       "\n",
       "     RIGHT_VENTRALDC_SUVR RIGHT_VENTRALDC_VOLUME RIGHT_VESSEL_SUVR  \\\n",
       "1                  1.6535                   4032            1.6759   \n",
       "3                  1.3132                   3689            1.1143   \n",
       "7                  1.6963                   4545            1.6395   \n",
       "15                 1.6008                   3464            1.6747   \n",
       "17                 1.7111                   3514            1.5387   \n",
       "...                   ...                    ...               ...   \n",
       "1337               1.3244                   3307            1.0539   \n",
       "1339               1.2943                   3942            1.3754   \n",
       "1341               1.2174                   3725            1.0669   \n",
       "1343               1.6743                   2620            1.4851   \n",
       "1345               1.2684                   3865            0.8988   \n",
       "\n",
       "      RIGHT_VESSEL_VOLUME WM_HYPOINTENSITIES_SUVR WM_HYPOINTENSITIES_VOLUME  \\\n",
       "1                    10.0                  1.8061                       783   \n",
       "3                    12.0                  1.4608                      1476   \n",
       "7                     3.0                  1.7398                      2185   \n",
       "15                    9.0                  1.6857                      6717   \n",
       "17                   34.0                  1.7918                      2820   \n",
       "...                   ...                     ...                       ...   \n",
       "1337                  5.0                  1.5795                       972   \n",
       "1339                  6.0                  1.3988                      3596   \n",
       "1341                 20.0                  1.2939                      8413   \n",
       "1343                  3.0                  1.2004                      6689   \n",
       "1345                  1.0                  1.3668                      2491   \n",
       "\n",
       "     TOTAL_INTRACRANIAL_VOLUME      update_stamp_y  \\\n",
       "1                 1.576281e+06 2021-01-19 10:55:04   \n",
       "3                 1.508126e+06 2021-01-19 10:55:04   \n",
       "7                 1.908584e+06 2021-01-19 10:55:04   \n",
       "15                1.552242e+06 2021-01-19 10:55:04   \n",
       "17                1.495694e+06 2021-01-19 10:55:04   \n",
       "...                        ...                 ...   \n",
       "1337              1.311141e+06 2021-01-19 10:55:04   \n",
       "1339              1.664610e+06 2021-01-19 10:55:04   \n",
       "1341              1.488525e+06 2021-01-19 10:55:05   \n",
       "1343              1.494192e+06 2021-01-19 10:55:05   \n",
       "1345              1.468981e+06 2021-01-19 10:55:06   \n",
       "\n",
       "     binary_labels_cross_section binary_labels_longitude  \n",
       "1                            2.0                     2.0  \n",
       "3                            0.0                     0.0  \n",
       "7                            1.0                     1.0  \n",
       "15                           1.0                     1.0  \n",
       "17                           1.0                     1.0  \n",
       "...                          ...                     ...  \n",
       "1337                         2.0                     2.0  \n",
       "1339                         2.0                     2.0  \n",
       "1341                         2.0                     2.0  \n",
       "1343                         1.0                     0.0  \n",
       "1345                         0.0                     0.0  \n",
       "\n",
       "[531 rows x 377 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "\n",
    "class_pairs = [['CN', 'SMC'], ['CN', 'SMC'],['EMCI', 'LMCI'],'AD']\n",
    "UCBERKLEY_PET = pd.read_excel('/home/hang/GU/Project/AD_classification_synthesis/data/ADNI/UCBERKELEYAV45_01_14_21.xlsx')\n",
    "dataTable = pd.read_csv('/home/hang/GitHub/BrainGNN_Pytorch/data/filtered_selectedDataUnique_merged_ADNI.csv')\n",
    "\n",
    "# Filter UCBERKLEY_PET for baseline rows if there is a specific column indicating baseline (assuming 'VISCODE' column here)\n",
    "UCBERKLEY_PET_bl = UCBERKLEY_PET[UCBERKLEY_PET['VISCODE2'] == 'bl']\n",
    "\n",
    "# Perform an inner join on the Subject column\n",
    "merged_data = pd.merge(dataTable, UCBERKLEY_PET_bl, left_on='RID', right_on='RID', how='inner').drop_duplicates(subset='Subject',keep='last')\n",
    "\n",
    "merged_data['binary_labels_cross_section'] = merged_data.apply(lambda row:\n",
    "            0 if row['DX_bl'] in class_pairs[0] and row['SUMMARYSUVR_WHOLECEREBNORM'] < 1.11 else\n",
    "            1 if row['DX_bl'] in class_pairs[1] and row['SUMMARYSUVR_WHOLECEREBNORM'] >= 1.11 else\n",
    "            2 if row['DX_bl'] in class_pairs[2] and row['SUMMARYSUVR_WHOLECEREBNORM'] >= 1.11 else\n",
    "            3 if row['DX_bl'] in class_pairs[3] and row['SUMMARYSUVR_WHOLECEREBNORM'] >= 1.11 else np.nan, axis=1)\n",
    "\n",
    "merged_data['binary_labels_longitude'] = merged_data.apply(lambda row:\n",
    "            0 if row['DX_bl'] in class_pairs[0] and row['SUMMARYSUVR_COMPOSITE_REFNORM'] < 0.79 else\n",
    "            1 if row['DX_bl'] in class_pairs[1] and row['SUMMARYSUVR_COMPOSITE_REFNORM'] >= 0.79 else\n",
    "            2 if row['DX_bl'] in class_pairs[2] and row['SUMMARYSUVR_COMPOSITE_REFNORM'] >= 0.79 else\n",
    "            3 if row['DX_bl'] in class_pairs[3] and row['SUMMARYSUVR_COMPOSITE_REFNORM'] >= 0.79 else np.nan, axis=1)\n",
    "\n",
    "filtered_data = merged_data.dropna(subset=['binary_labels_cross_section'])\n",
    "# filtered_data = filtered_data.dropna(subset=['binary_labels_longitude'])\n",
    "filtered_data= filtered_data.drop_duplicates(subset='Subject', keep='last')\n",
    "merged_data.to_csv('merged_filtered_selected_data_ADNI_merge_with_UCBERKELEYAV45_01_14_21.csv')\n",
    "filtered_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import timeit\n",
    "import networkx as nx\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import networkx as nx\n",
    "from scipy.io import loadmat\n",
    "from torch_geometric.data import Data,  Batch\n",
    "from torch_geometric.utils import  remove_self_loops\n",
    "import multiprocessing\n",
    "from torch_sparse import coalesce\n",
    "from functools import partial\n",
    "def extract_features(file_path, field_name='scaledMahalDistMatrix'):\n",
    "    \"\"\"\n",
    "    Placeholder function for extracting features from a .mat file.\n",
    "    Implement according to your specific requirements.\n",
    "    \"\"\"\n",
    "    mat = loadmat(file_path)\n",
    "    return mat[field_name]\n",
    "\n",
    "def apply_threshold(matrix, percentile):\n",
    "    # Flatten the matrix and sort it\n",
    "    sorted_matrix = np.sort(matrix)\n",
    "    # Calculate the index for the desired percentile\n",
    "    threshold_index = round(len(sorted_matrix) * (100 - percentile) / 100)  # Adjust for zero indexing\n",
    "\n",
    "    # Get the threshold value using the sorted matrix\n",
    "    threshold_value = sorted_matrix[threshold_index]\n",
    "    # Apply thresholding\n",
    "    thresholded_matrix = np.where(matrix > threshold_value, matrix, 0)\n",
    "    return thresholded_matrix\n",
    "\n",
    "\n",
    "def process_subject(subject_id, binary_label, mat_files_dir, percentile, connectome):\n",
    "    \"\"\"\n",
    "    Process a single subject's graph data for classification, including binary label.\n",
    "    \"\"\"\n",
    "    mat_file_path = os.path.join(mat_files_dir, f\"{subject_id}_{connectome}.mat\")\n",
    "    par_corr_file_path = os.path.join(mat_files_dir, f\"{subject_id}_partial_correlation_KDE.mat\")\n",
    "\n",
    "    # Check if files exist\n",
    "    if not os.path.exists(mat_file_path) or not os.path.exists(par_corr_file_path):\n",
    "        print(f\"Files for subject {subject_id} are missing. Paths: {mat_file_path}, {par_corr_file_path}\")\n",
    "        return None\n",
    "    \n",
    "    # Determine the field name based on connectome type\n",
    "    field_name = {\n",
    "        'ScaledMahalanobisDistanceMatrix': 'scaledMahalDistMatrix',\n",
    "        'Z_scoring': 'zScoreConnectivityMatrix',\n",
    "        'K_correlation': 'correlationMatrix',\n",
    "        'JSdivMatrix': 'K_JS_Divergence'\n",
    "    }.get(connectome, None)\n",
    "    \n",
    "\n",
    "    print(f\"Processing subject {subject_id} with connectome: {connectome}\")\n",
    "\n",
    "    # Extract features\n",
    "    node_features = apply_threshold(extract_features(mat_file_path, field_name=field_name), percentile=percentile)\n",
    "    edge_features = apply_threshold(extract_features(par_corr_file_path, field_name='partial_corr'), percentile=percentile)\n",
    "\n",
    "\n",
    "    # Construct graph\n",
    "    num_nodes = node_features.shape[0]\n",
    "    G = nx.from_numpy_array(edge_features)\n",
    "    A = nx.to_scipy_sparse_array(G, format='coo')\n",
    "    adj = A.tocoo()\n",
    "\n",
    "    # Create edge attributes\n",
    "    edge_att = np.zeros(len(adj.row))\n",
    "    for i in range(len(adj.row)):\n",
    "        edge_att[i] = node_features[adj.row[i], adj.col[i]]\n",
    "\n",
    "    edge_index = np.stack([A.row, A.col])\n",
    "    edge_index, edge_att = remove_self_loops(torch.from_numpy(edge_index), torch.from_numpy(edge_att))\n",
    "    edge_index = edge_index.long()\n",
    "    edge_index, edge_att = coalesce(edge_index, edge_att, num_nodes, num_nodes)\n",
    "    pos = torch.eye(num_nodes)  # Using an identity matrix for positional data\n",
    "\n",
    "    return Data(x=torch.tensor(node_features, dtype=torch.float),\n",
    "                edge_index=edge_index,\n",
    "                edge_attr=edge_att,\n",
    "                y=torch.tensor([binary_label], dtype=torch.long),\n",
    "                pos=pos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# List to store processed data\n",
    "all_data = []\n",
    "\n",
    "# Directory containing the .mat files\n",
    "mat_files_dir = \"/media/hang/EXTERNAL_US/Data/1_HANG_FDG_PET/ADNI_Second_organized/KDE_Results_corrected_by_age_sec_education/\"\n",
    "\n",
    "# Path to the raw data\n",
    "data_list_included = glob.glob('/media/hang/EXTERNAL_US/Data/1_HANG_FDG_PET/ADNI_Second_organized/KDE_Results/raw/*meanSUVR.mat')\n",
    "\n",
    "# Percentile parameter\n",
    "percentile = 10\n",
    "\n",
    "# Extract subject IDs from file names\n",
    "subject_ids_included = [filename.split('/')[-1].split('_meanSUVR')[0] for filename in data_list_included]\n",
    "\n",
    "# Assuming ADNI_merge and UCBERKLEY_PET are already loaded DataFrames\n",
    "# Example class_pairs definition\n",
    "class_pairs = [[\"CN\", \"SMC\"], ['MCI', 'EMCI', 'LMCI'], [\"Dementia\"]]\n",
    "connectome = 'Z_scoring'\n",
    "\n",
    "for id in subject_ids_included:\n",
    "    # Get the diagnosis values sorted by exam date for the subject\n",
    "    diagnosis_values = ADNI_merge[ADNI_merge['PTID'] == id].sort_values('EXAMDATE')['DX'].dropna().values\n",
    "    \n",
    "    # Check if there is more than one unique diagnosis\n",
    "    if len(set(diagnosis_values)) > 1:\n",
    "        continue\n",
    "\n",
    "    # Extract RID from the subject ID\n",
    "    rid = id.split('_')[-1]\n",
    "    print(f\"Extracted RID: {rid}\")\n",
    "\n",
    "    # Check the SUVR values for the extracted RID\n",
    "    suvr_values = UCBERKLEY_PET[UCBERKLEY_PET['RID'].astype(str) == rid]['SUMMARYSUVR_COMPOSITE_REFNORM']\n",
    "    \n",
    "    # Skip if SUVR values are empty or have mixed values for threshold\n",
    "    if suvr_values.empty or len(set(suvr_values > 0.79)) > 1:\n",
    "        continue\n",
    "\n",
    "    # Skip if there are fewer than 2 diagnosis values or SUVR values\n",
    "    if len(diagnosis_values) < 2 or len(suvr_values) < 2:\n",
    "        continue\n",
    "\n",
    "    # Determine the diagnosis and amyloid-beta status\n",
    "    diagnosis = diagnosis_values[0]\n",
    "    ab = (suvr_values > 0.79).astype(int).values[0]\n",
    "\n",
    "    # Set binary_label based on conditions\n",
    "    binary_label = np.nan\n",
    "    if diagnosis in class_pairs[0]:\n",
    "        binary_label = 0 if ab == 0 else 1\n",
    "    elif diagnosis in class_pairs[1] and ab == 1:\n",
    "        binary_label = 2\n",
    "    elif diagnosis in class_pairs[2] and ab == 1:\n",
    "        binary_label = 3\n",
    "    else:\n",
    "        continue\n",
    "    \n",
    "    # Process the subject data\n",
    "    data = process_subject(id, float(binary_label), mat_files_dir, percentile=percentile, connectome=connectome)\n",
    "    \n",
    "    # Append the processed data if not None\n",
    "    if data is not None:\n",
    "        all_data.append(data)\n",
    "\n",
    "\n",
    "# Print the number of processed data entries\n",
    "print(f\"Total processed data entries: {len(all_data)}\")\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of intersecting subjects: 588\n",
      "binary_labels_cross_section\n",
      "0.0    148\n",
      "2.0    144\n",
      "3.0    105\n",
      "1.0     79\n",
      "Name: count, dtype: int64\n",
      "binary_labels_longitude\n",
      "0.0    164\n",
      "2.0    136\n",
      "3.0    103\n",
      "1.0     63\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "466"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert lists to sets for intersection\n",
    "data_list_included = glob.glob('/media/hang/EXTERNAL_US/Data/1_HANG_FDG_PET/ADNI_Second_organized/KDE_Results/raw/*meanSUVR.mat')\n",
    "subject_ids_included = [filename.split('/')[-1].split('_meanSUVR')[0] for filename in data_list_included]\n",
    "set_subject_ids_included = set(subject_ids_included)\n",
    "set_filtered_subjects = set(merged_data['Subject'])\n",
    "\n",
    "\n",
    "\n",
    "# Find intersection\n",
    "intersection_subjects_bl = set_subject_ids_included.intersection(set_filtered_subjects)\n",
    "analized_data_included_bl = merged_data[merged_data['Subject'].isin(intersection_subjects_bl)]\n",
    "\n",
    "# Print intersection result\n",
    "print(f\"Number of intersecting subjects: {len(analized_data_included_bl)}\")\n",
    "\n",
    "\n",
    "print(analized_data_included_bl.value_counts(subset='binary_labels_cross_section'))\n",
    "print(analized_data_included_bl.value_counts(subset='binary_labels_longitude'))\n",
    "union_set_longitudinal = list(union_set_longitudinal)\n",
    "#filtered_data_cross_sectional_bl_longitudinal = union_set_longitudinal[union_set_longitudinal in (analized_data_included_bl['Subject'])]\n",
    "#list(set(analized_data_included_bl['Subject']) & set(union_set_longitudinal))\n",
    "## bl subjects count\n",
    "#analized_data_included_bl[analized_data_included_bl['binary_labels_cross_section']==0]['Subject']\n",
    "analized_data_included_bl\n",
    "164+136+103+63"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "subjects_not_in_filtered =  set_subject_ids_included  - set_filtered_subjects\n",
    "print(\"Subjects in set_subject_ids_included but not in set_filtered_subjects:\")\n",
    "print(len(subjects_not_in_filtered))\n",
    "subjects_not_in_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import csv\n",
    "# Filter and get the list of subjects where binary_labels is 0\n",
    "subjects_list_CN_neg = analized_data_included_bl[analized_data_included_bl['binary_labels_cross_section'] == 0]['Subject']\n",
    "subjects_list_CN_pos = analized_data_included_bl[analized_data_included_bl['binary_labels_cross_section'] == 1]['Subject']\n",
    "subjects_list_MCI_pos = analized_data_included_bl[analized_data_included_bl['binary_labels_cross_section'] == 2]['Subject']\n",
    "subjects_list_AD_pos = analized_data_included_bl[analized_data_included_bl['binary_labels_cross_section'] == 3]['Subject']\n",
    "\n",
    "# Convert the series to a list of strings\n",
    "subjects_str_list_CN_neg = [str(subject) + ',' for subject in subjects_list_CN_neg]\n",
    "subjects_str_list_CN_pos = [str(subject) + ',' for subject in subjects_list_CN_pos]\n",
    "subjects_str_list_MCI_pos = [str(subject) + ',' for subject in subjects_list_MCI_pos]\n",
    "subjects_str_list_AD_pos = [str(subject) + ',' for subject in subjects_list_AD_pos]\n",
    "\n",
    "# Function to save subjects' lists to CSV files, each subject on a new line\n",
    "def save_subjects_to_csv(filename, header, subjects_list):\n",
    "    with open(filename, 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow([header])  # Write the header\n",
    "        for subject in subjects_list:\n",
    "            writer.writerow([subject])  # Write each subject in a new line\n",
    "\n",
    "# Save each list of subjects to separate CSV files\n",
    "save_subjects_to_csv('/media/hang/EXTERNAL_US/Data/1_HANG_FDG_PET/longitudinal_AD_MCI_CN/longitudinalthres1.11_subjects_cn_neg.csv',\n",
    "                     'CN_neg', subjects_str_list_CN_neg)\n",
    "\n",
    "save_subjects_to_csv('/media/hang/EXTERNAL_US/Data/1_HANG_FDG_PET/longitudinal_AD_MCI_CN/longitudinalthres1.11_subjects_cn_pos.csv',\n",
    "                     'CN_Pos', subjects_str_list_CN_pos)\n",
    "\n",
    "save_subjects_to_csv('/media/hang/EXTERNAL_US/Data/1_HANG_FDG_PET/longitudinal_AD_MCI_CN/longitudinalthres1.11_subjects_mci_pos.csv',\n",
    "                     'MCI_Pos', subjects_str_list_MCI_pos)\n",
    "\n",
    "save_subjects_to_csv('/media/hang/EXTERNAL_US/Data/1_HANG_FDG_PET/longitudinal_AD_MCI_CN/longitudinalthres1.11_subjects_ad_pos.csv',\n",
    "                     'AD_Pos', subjects_str_list_AD_pos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_pos = list(subjects_str_list_CN_pos + subjects_str_list_MCI_pos + subjects_str_list_AD_pos)\n",
    "print(len(set(subjects_str_list_CN_neg)))\n",
    "print('ab+ overlapped cases',len(set(subject_pos) & set(ab_pos_cases)))# see the overlap of bl with longitudinal downloaded\n",
    "\n",
    "subject_neg = subjects_str_list_CN_neg\n",
    "print(set(subject_neg).intersection(set(subject_pos)))\n",
    "print('ab- overlapped cases',len(set(subject_neg) & set(ab_neg_cases)))\n",
    "print(len(set(hang_FDG_ab_pos['Subject'])))\n",
    "print(len(set(subject_pos).intersection(hang_FDG_ab_pos['Subject'])))\n",
    "print(len(set(subject_neg).intersection(hang_FDG_CN_ab_neg['Subject'])))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Precentral_L': 1, 'Precentral_R': 2, 'Frontal_Sup_2_L': 3, 'Frontal_Sup_2_R': 4, 'Frontal_Mid_2_L': 5, 'Frontal_Mid_2_R': 6, 'Frontal_Inf_Oper_L': 7, 'Frontal_Inf_Oper_R': 8, 'Frontal_Inf_Tri_L': 9, 'Frontal_Inf_Tri_R': 10, 'Frontal_Inf_Orb_2_L': 11, 'Frontal_Inf_Orb_2_R': 12, 'Rolandic_Oper_L': 13, 'Rolandic_Oper_R': 14, 'Supp_Motor_Area_L': 15, 'Supp_Motor_Area_R': 16, 'Olfactory_L': 17, 'Olfactory_R': 18, 'Frontal_Sup_Medial_L': 19, 'Frontal_Sup_Medial_R': 20, 'Frontal_Med_Orb_L': 21, 'Frontal_Med_Orb_R': 22, 'Rectus_L': 23, 'Rectus_R': 24, 'OFCmed_L': 25, 'OFCmed_R': 26, 'OFCant_L': 27, 'OFCant_R': 28, 'OFCpost_L': 29, 'OFCpost_R': 30, 'OFClat_L': 31, 'OFClat_R': 32, 'Insula_L': 33, 'Insula_R': 34, 'Cingulate_Ant_L': 35, 'Cingulate_Ant_R': 36, 'Cingulate_Mid_L': 37, 'Cingulate_Mid_R': 38, 'Cingulate_Post_L': 39, 'Cingulate_Post_R': 40, 'Hippocampus_L': 41, 'Hippocampus_R': 42, 'ParaHippocampal_L': 43, 'ParaHippocampal_R': 44, 'Amygdala_L': 45, 'Amygdala_R': 46, 'Calcarine_L': 47, 'Calcarine_R': 48, 'Cuneus_L': 49, 'Cuneus_R': 50, 'Lingual_L': 51, 'Lingual_R': 52, 'Occipital_Sup_L': 53, 'Occipital_Sup_R': 54, 'Occipital_Mid_L': 55, 'Occipital_Mid_R': 56, 'Occipital_Inf_L': 57, 'Occipital_Inf_R': 58, 'Fusiform_L': 59, 'Fusiform_R': 60, 'Postcentral_L': 61, 'Postcentral_R': 62, 'Parietal_Sup_L': 63, 'Parietal_Sup_R': 64, 'Parietal_Inf_L': 65, 'Parietal_Inf_R': 66, 'SupraMarginal_L': 67, 'SupraMarginal_R': 68, 'Angular_L': 69, 'Angular_R': 70, 'Precuneus_L': 71, 'Precuneus_R': 72, 'Paracentral_Lobule_L': 73, 'Paracentral_Lobule_R': 74, 'Caudate_L': 75, 'Caudate_R': 76, 'Putamen_L': 77, 'Putamen_R': 78, 'Pallidum_L': 79, 'Pallidum_R': 80, 'Thalamus_L': 81, 'Thalamus_R': 82, 'Heschl_L': 83, 'Heschl_R': 84, 'Temporal_Sup_L': 85, 'Temporal_Sup_R': 86, 'Temporal_Pole_Sup_L': 87, 'Temporal_Pole_Sup_R': 88, 'Temporal_Mid_L': 89, 'Temporal_Mid_R': 90, 'Temporal_Pole_Mid_L': 91, 'Temporal_Pole_Mid_R': 92, 'Temporal_Inf_L': 93, 'Temporal_Inf_R': 94, 'Cerebelum_Crus1_L': 95, 'Cerebelum_Crus1_R': 96, 'Cerebelum_Crus2_L': 97, 'Cerebelum_Crus2_R': 98, 'Cerebelum_3_L': 99, 'Cerebelum_3_R': 100, 'Cerebelum_4_5_L': 101, 'Cerebelum_4_5_R': 102, 'Cerebelum_6_L': 103, 'Cerebelum_6_R': 104, 'Cerebelum_7b_L': 105, 'Cerebelum_7b_R': 106, 'Cerebelum_8_L': 107, 'Cerebelum_8_R': 108, 'Cerebelum_9_L': 109, 'Cerebelum_9_R': 110, 'Cerebelum_10_L': 111, 'Cerebelum_10_R': 112, 'Vermis_1_2': 113, 'Vermis_3': 114, 'Vermis_4_5': 115, 'Vermis_6': 116, 'Vermis_7': 117, 'Vermis_8': 118, 'Vermis_9': 119, 'Vermis_10': 120}\n"
     ]
    }
   ],
   "source": [
    "regions_dict = {\n",
    "    \"Precentral_L\": 1,\n",
    "    \"Precentral_R\": 2,\n",
    "    \"Frontal_Sup_2_L\": 3,\n",
    "    \"Frontal_Sup_2_R\": 4,\n",
    "    \"Frontal_Mid_2_L\": 5,\n",
    "    \"Frontal_Mid_2_R\": 6,\n",
    "    \"Frontal_Inf_Oper_L\": 7,\n",
    "    \"Frontal_Inf_Oper_R\": 8,\n",
    "    \"Frontal_Inf_Tri_L\": 9,\n",
    "    \"Frontal_Inf_Tri_R\": 10,\n",
    "    \"Frontal_Inf_Orb_2_L\": 11,\n",
    "    \"Frontal_Inf_Orb_2_R\": 12,\n",
    "    \"Rolandic_Oper_L\": 13,\n",
    "    \"Rolandic_Oper_R\": 14,\n",
    "    \"Supp_Motor_Area_L\": 15,\n",
    "    \"Supp_Motor_Area_R\": 16,\n",
    "    \"Olfactory_L\": 17,\n",
    "    \"Olfactory_R\": 18,\n",
    "    \"Frontal_Sup_Medial_L\": 19,\n",
    "    \"Frontal_Sup_Medial_R\": 20,\n",
    "    \"Frontal_Med_Orb_L\": 21,\n",
    "    \"Frontal_Med_Orb_R\": 22,\n",
    "    \"Rectus_L\": 23,\n",
    "    \"Rectus_R\": 24,\n",
    "    \"OFCmed_L\": 25,\n",
    "    \"OFCmed_R\": 26,\n",
    "    \"OFCant_L\": 27,\n",
    "    \"OFCant_R\": 28,\n",
    "    \"OFCpost_L\": 29,\n",
    "    \"OFCpost_R\": 30,\n",
    "    \"OFClat_L\": 31,\n",
    "    \"OFClat_R\": 32,\n",
    "    \"Insula_L\": 33,\n",
    "    \"Insula_R\": 34,\n",
    "    \"Cingulate_Ant_L\": 35,\n",
    "    \"Cingulate_Ant_R\": 36,\n",
    "    \"Cingulate_Mid_L\": 37,\n",
    "    \"Cingulate_Mid_R\": 38,\n",
    "    \"Cingulate_Post_L\": 39,\n",
    "    \"Cingulate_Post_R\": 40,\n",
    "    \"Hippocampus_L\": 41,\n",
    "    \"Hippocampus_R\": 42,\n",
    "    \"ParaHippocampal_L\": 43,\n",
    "    \"ParaHippocampal_R\": 44,\n",
    "    \"Amygdala_L\": 45,\n",
    "    \"Amygdala_R\": 46,\n",
    "    \"Calcarine_L\": 47,\n",
    "    \"Calcarine_R\": 48,\n",
    "    \"Cuneus_L\": 49,\n",
    "    \"Cuneus_R\": 50,\n",
    "    \"Lingual_L\": 51,\n",
    "    \"Lingual_R\": 52,\n",
    "    \"Occipital_Sup_L\": 53,\n",
    "    \"Occipital_Sup_R\": 54,\n",
    "    \"Occipital_Mid_L\": 55,\n",
    "    \"Occipital_Mid_R\": 56,\n",
    "    \"Occipital_Inf_L\": 57,\n",
    "    \"Occipital_Inf_R\": 58,\n",
    "    \"Fusiform_L\": 59,\n",
    "    \"Fusiform_R\": 60,\n",
    "    \"Postcentral_L\": 61,\n",
    "    \"Postcentral_R\": 62,\n",
    "    \"Parietal_Sup_L\": 63,\n",
    "    \"Parietal_Sup_R\": 64,\n",
    "    \"Parietal_Inf_L\": 65,\n",
    "    \"Parietal_Inf_R\": 66,\n",
    "    \"SupraMarginal_L\": 67,\n",
    "    \"SupraMarginal_R\": 68,\n",
    "    \"Angular_L\": 69,\n",
    "    \"Angular_R\": 70,\n",
    "    \"Precuneus_L\": 71,\n",
    "    \"Precuneus_R\": 72,\n",
    "    \"Paracentral_Lobule_L\": 73,\n",
    "    \"Paracentral_Lobule_R\": 74,\n",
    "    \"Caudate_L\": 75,\n",
    "    \"Caudate_R\": 76,\n",
    "    \"Putamen_L\": 77,\n",
    "    \"Putamen_R\": 78,\n",
    "    \"Pallidum_L\": 79,\n",
    "    \"Pallidum_R\": 80,\n",
    "    \"Thalamus_L\": 81,\n",
    "    \"Thalamus_R\": 82,\n",
    "    \"Heschl_L\": 83,\n",
    "    \"Heschl_R\": 84,\n",
    "    \"Temporal_Sup_L\": 85,\n",
    "    \"Temporal_Sup_R\": 86,\n",
    "    \"Temporal_Pole_Sup_L\": 87,\n",
    "    \"Temporal_Pole_Sup_R\": 88,\n",
    "    \"Temporal_Mid_L\": 89,\n",
    "    \"Temporal_Mid_R\": 90,\n",
    "    \"Temporal_Pole_Mid_L\": 91,\n",
    "    \"Temporal_Pole_Mid_R\": 92,\n",
    "    \"Temporal_Inf_L\": 93,\n",
    "    \"Temporal_Inf_R\": 94,\n",
    "    \"Cerebelum_Crus1_L\": 95,\n",
    "    \"Cerebelum_Crus1_R\": 96,\n",
    "    \"Cerebelum_Crus2_L\": 97,\n",
    "    \"Cerebelum_Crus2_R\": 98,\n",
    "    \"Cerebelum_3_L\": 99,\n",
    "    \"Cerebelum_3_R\": 100,\n",
    "    \"Cerebelum_4_5_L\": 101,\n",
    "    \"Cerebelum_4_5_R\": 102,\n",
    "    \"Cerebelum_6_L\": 103,\n",
    "    \"Cerebelum_6_R\": 104,\n",
    "    \"Cerebelum_7b_L\": 105,\n",
    "    \"Cerebelum_7b_R\": 106,\n",
    "    \"Cerebelum_8_L\": 107,\n",
    "    \"Cerebelum_8_R\": 108,\n",
    "    \"Cerebelum_9_L\": 109,\n",
    "    \"Cerebelum_9_R\": 110,\n",
    "    \"Cerebelum_10_L\": 111,\n",
    "    \"Cerebelum_10_R\": 112,\n",
    "    \"Vermis_1_2\": 113,\n",
    "    \"Vermis_3\": 114,\n",
    "    \"Vermis_4_5\": 115,\n",
    "    \"Vermis_6\": 116,\n",
    "    \"Vermis_7\": 117,\n",
    "    \"Vermis_8\": 118,\n",
    "    \"Vermis_9\": 119,\n",
    "    \"Vermis_10\": 120\n",
    "}\n",
    "\n",
    "print(regions_dict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 1, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n",
      "[35, 38, 48, 27, 0, 2, 11, 12, 14, 15, 30, 46, 47, 48, 49, 50, 51, 57]\n",
      "[0, 0, 1, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n",
      "[35, 38, 48, 27, 0, 2, 11, 12, 14, 15, 30, 46, 47, 48, 49, 50, 51, 57]\n",
      "[0, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n",
      "[38, 48, 0, 2, 3, 6, 11, 12, 14, 15, 46, 47, 48, 49, 50, 51]\n",
      "[0, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n",
      "[38, 48, 0, 2, 3, 6, 11, 12, 14, 15, 46, 47, 48, 49, 50, 51]\n",
      "[0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n",
      "[38, 0, 2, 3, 6, 11, 12, 13, 14, 15, 46, 47, 48, 49, 50, 51]\n",
      "[3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n",
      "[0, 2, 3, 6, 11, 12, 14, 15, 46, 47, 48, 49, 50, 51]\n",
      "[3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n",
      "[0, 2, 3, 6, 11, 12, 14, 15, 46, 47, 48, 49, 50, 51]\n",
      "[['Cingulate_Ant_R', 'Cingulate_Post_L'], ['Cuneus_L'], ['OFCant_R'], ['Precentral_L', 'Frontal_Sup_2_L', 'Frontal_Sup_2_R', 'Frontal_Inf_Oper_L', 'Frontal_Inf_Orb_2_R', 'Rolandic_Oper_L', 'Rolandic_Oper_R', 'Supp_Motor_Area_L', 'Supp_Motor_Area_R', 'OFClat_L', 'Calcarine_L', 'Calcarine_R', 'Cuneus_L', 'Cuneus_R', 'Lingual_L', 'Lingual_R', 'Occipital_Inf_R']]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "# Define the variables\n",
    "date_time = '2024-06-03_11-07-03'\n",
    "z_scoring = 'ScaledMahalanobisDistanceMatrix'\n",
    "percentile = 20\n",
    "class_pair1 = ['CN', 'SMC']\n",
    "class_pair2 = ['MCI', 'EMCI', 'LMCI']\n",
    "# ['CN', 'SMC']#['MCI', 'EMCI', 'LMCI']#'Dementia'\n",
    "\n",
    "# Construct class pair strings with single quotes\n",
    "class_pair1_str = ', '.join([f\"'{c}'\" for c in class_pair1])\n",
    "\n",
    "class_pair2_str = ', '.join([f\"'{c}'\" for c in class_pair2])\n",
    "# Construct the file path\n",
    "file_path = f\"/home/hang/GitHub/BrainGNN_Pytorch/results/{date_time}/{z_scoring}/{percentile}/[{class_pair1_str}]_vs_{class_pair2}/results/interesting_indices.json\"\n",
    "\n",
    "# Open the file\n",
    "with open(file_path, 'r') as f:\n",
    "    indices = json.load(f)\n",
    "import numpy as np\n",
    "import ast\n",
    "\n",
    "# Assuming 'indices' is a dictionary loaded from the JSON file\n",
    "\n",
    "community_clustering = {0: [], 1: [], 2: [], 3: []}\n",
    "node_number_past = -1\n",
    "# Iterate through each fold and its associated nodes\n",
    "for fold, nodes in indices.items():\n",
    "    fold = ast.literal_eval(fold)  # Convert the fold string to an actual object (e.g., tuple)\n",
    "    \n",
    "    # Iterate through each batch in the list of nodes\n",
    "    for batch in range(len(nodes)):\n",
    "        # Convert the string representation of the lists to actual lists\n",
    "        community_list = ast.literal_eval(nodes[batch][0])\n",
    "        print(community_list)  # Debugging output, can be removed in production\n",
    "        \n",
    "        node_list = ast.literal_eval(nodes[batch][1])\n",
    "        print(node_list)  # Debugging output, can be removed in production\n",
    "        \n",
    "        # Convert the community list to a numpy array of integers\n",
    "        community = np.array(community_list, dtype=int)\n",
    "        \n",
    "        # Create iterators for communities and nodes\n",
    "        community_iter = iter(community)\n",
    "        community_number = next(community_iter, None)  # Start with the first community number\n",
    "\n",
    "        node_iter = iter(node_list)\n",
    "        node_number = next(node_iter, None)  # Start with the first node number\n",
    "        \n",
    "        # Iterate through the communities and nodes and append the nodes to their respective communities\n",
    "        while community_number is not None and node_number is not None:\n",
    "            community_clustering[community_number].append(node_number)\n",
    "            community_number = next(community_iter, None)\n",
    "            node_number = next(node_iter, None)\n",
    "\n",
    "    \n",
    "for keys, values in community_clustering.items():\n",
    "    # Replace the list of values with a unique set of values\n",
    "    community_clustering[keys] = np.unique(values).tolist()  # Convert to list if necessary\n",
    "\n",
    "\n",
    "\n",
    "# Reverse the regions_dict to map indices to region names\n",
    "index_to_region = {v-1: k for k, v in regions_dict.items()}\n",
    "\n",
    "# Create a new dictionary with region names instead of numeric indices\n",
    "community_clustering_with_names = []\n",
    "for community_number, nodes in community_clustering.items():\n",
    "\n",
    "    region_name = [index_to_region.get(node, f\"Unknown_{community_number}\") for node in nodes]\n",
    "    community_clustering_with_names.append(region_name)\n",
    "\n",
    "print(community_clustering_with_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from nilearn import plotting, image\n",
    "from nilearn.image import math_img, resample_to_img,new_img_like\n",
    "from collections import defaultdict\n",
    "\n",
    "# Load the AAL2 atlas from the local path\n",
    "path_to_aal2 = '/home/hang/dsi-studio/atlas/ICBM152_adult/AAL2.nii.gz'\n",
    "aal_atlas = image.load_img(path_to_aal2)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from nilearn import plotting\n",
    "from nilearn.image import new_img_like, math_img\n",
    "import matplotlib as mpl\n",
    "# Provided data for constructing regions_dict\n",
    "regions_data = \"\"\"\n",
    "Precentral_L 1 2001\n",
    "Precentral_R 2 2002\n",
    "Frontal_Sup_2_L 3 2101\n",
    "Frontal_Sup_2_R 4 2102\n",
    "Frontal_Mid_2_L 5 2201\n",
    "Frontal_Mid_2_R 6 2202\n",
    "Frontal_Inf_Oper_L 7 2301\n",
    "Frontal_Inf_Oper_R 8 2302\n",
    "Frontal_Inf_Tri_L 9 2311\n",
    "Frontal_Inf_Tri_R 10 2312\n",
    "Frontal_Inf_Orb_2_L 11 2321\n",
    "Frontal_Inf_Orb_2_R 12 2322\n",
    "Rolandic_Oper_L 13 2331\n",
    "Rolandic_Oper_R 14 2332\n",
    "Supp_Motor_Area_L 15 2401\n",
    "Supp_Motor_Area_R 16 2402\n",
    "Olfactory_L 17 2501\n",
    "Olfactory_R 18 2502\n",
    "Frontal_Sup_Medial_L 19 2601\n",
    "Frontal_Sup_Medial_R 20 2602\n",
    "Frontal_Med_Orb_L 21 2611\n",
    "Frontal_Med_Orb_R 22 2612\n",
    "Rectus_L 23 2701\n",
    "Rectus_R 24 2702\n",
    "OFCmed_L 25 2801\n",
    "OFCmed_R 26 2802\n",
    "OFCant_L 27 2811\n",
    "OFCant_R 28 2812\n",
    "OFCpost_L 29 2821\n",
    "OFCpost_R 30 2822\n",
    "OFClat_L 31 2831\n",
    "OFClat_R 32 2832\n",
    "Insula_L 33 3001\n",
    "Insula_R 34 3002\n",
    "Cingulate_Ant_L 35 4001\n",
    "Cingulate_Ant_R 36 4002\n",
    "Cingulate_Mid_L 37 4011\n",
    "Cingulate_Mid_R 38 4012\n",
    "Cingulate_Post_L 39 4021\n",
    "Cingulate_Post_R 40 4022\n",
    "Hippocampus_L 41 4101\n",
    "Hippocampus_R 42 4102\n",
    "ParaHippocampal_L 43 4111\n",
    "ParaHippocampal_R 44 4112\n",
    "Amygdala_L 45 4201\n",
    "Amygdala_R 46 4202\n",
    "Calcarine_L 47 5001\n",
    "Calcarine_R 48 5002\n",
    "Cuneus_L 49 5011\n",
    "Cuneus_R 50 5012\n",
    "Lingual_L 51 5021\n",
    "Lingual_R 52 5022\n",
    "Occipital_Sup_L 53 5101\n",
    "Occipital_Sup_R 54 5102\n",
    "Occipital_Mid_L 55 5201\n",
    "Occipital_Mid_R 56 5202\n",
    "Occipital_Inf_L 57 5301\n",
    "Occipital_Inf_R 58 5302\n",
    "Fusiform_L 59 5401\n",
    "Fusiform_R 60 5402\n",
    "Postcentral_L 61 6001\n",
    "Postcentral_R 62 6002\n",
    "Parietal_Sup_L 63 6101\n",
    "Parietal_Sup_R 64 6102\n",
    "Parietal_Inf_L 65 6201\n",
    "Parietal_Inf_R 66 6202\n",
    "SupraMarginal_L 67 6211\n",
    "SupraMarginal_R 68 6212\n",
    "Angular_L 69 6221\n",
    "Angular_R 70 6222\n",
    "Precuneus_L 71 6301\n",
    "Precuneus_R 72 6302\n",
    "Paracentral_Lobule_L 73 6401\n",
    "Paracentral_Lobule_R 74 6402\n",
    "Caudate_L 75 7001\n",
    "Caudate_R 76 7002\n",
    "Putamen_L 77 7011\n",
    "Putamen_R 78 7012\n",
    "Pallidum_L 79 7021\n",
    "Pallidum_R 80 7022\n",
    "Thalamus_L 81 7101\n",
    "Thalamus_R 82 7102\n",
    "Heschl_L 83 8101\n",
    "Heschl_R 84 8102\n",
    "Temporal_Sup_L 85 8111\n",
    "Temporal_Sup_R 86 8112\n",
    "Temporal_Pole_Sup_L 87 8121\n",
    "Temporal_Pole_Sup_R 88 8122\n",
    "Temporal_Mid_L 89 8201\n",
    "Temporal_Mid_R 90 8202\n",
    "Temporal_Pole_Mid_L 91 8211\n",
    "Temporal_Pole_Mid_R 92 8212\n",
    "Temporal_Inf_L 93 8301\n",
    "Temporal_Inf_R 94 8302\n",
    "Cerebelum_Crus1_L 95 9001\n",
    "Cerebelum_Crus1_R 96 9002\n",
    "Cerebelum_Crus2_L 97 9011\n",
    "Cerebelum_Crus2_R 98 9012\n",
    "Cerebelum_3_L 99 9021\n",
    "Cerebelum_3_R 100 9022\n",
    "Cerebelum_4_5_L 101 9031\n",
    "Cerebelum_4_5_R 102 9032\n",
    "Cerebelum_6_L 103 9041\n",
    "Cerebelum_6_R 104 9042\n",
    "Cerebelum_7b_L 105 9051\n",
    "Cerebelum_7b_R 106 9052\n",
    "Cerebelum_8_L 107 9061\n",
    "Cerebelum_8_R 108 9062\n",
    "Cerebelum_9_L 109 9071\n",
    "Cerebelum_9_R 110 9072\n",
    "Cerebelum_10_L 111 9081\n",
    "Cerebelum_10_R 112 9082\n",
    "Vermis_1_2 113 9100\n",
    "Vermis_3 114 9110\n",
    "Vermis_4_5 115 9120\n",
    "Vermis_6 116 9130\n",
    "Vermis_7 117 9140\n",
    "Vermis_8 118 9150\n",
    "Vermis_9 119 9160\n",
    "Vermis_10 120 9170\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Parse the data into a dictionary\n",
    "regions_dict = {}\n",
    "for line in regions_data.strip().split(\"\\n\"):\n",
    "    parts = line.split()\n",
    "    region_name = parts[0]\n",
    "    atlas_index = int(parts[2])\n",
    "    regions_dict[region_name] = atlas_index\n",
    "\n",
    "# Function to create a binary mask for the given region indices\n",
    "def create_region_mask(atlas_img, region_indices):\n",
    "    mask = np.zeros(atlas_img.shape, dtype=bool)\n",
    "    for index in region_indices:\n",
    "        region_mask = math_img(f'img == {index}', img=atlas_img)\n",
    "        mask |= region_mask.get_fdata().astype(bool)\n",
    "    return new_img_like(atlas_img, mask)\n",
    "\n",
    "# Plot regions for each community\n",
    "n_communities = len(community_clustering_with_names)\n",
    "fig, axes = plt.subplots(2, (n_communities + 1) // 2, figsize=(20, 10))\n",
    "\n",
    "if n_communities == 1:\n",
    "    axes = np.array([axes])  # Ensure axes is iterable\n",
    "\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Assuming aal_atlas is predefined as the atlas image\n",
    "for ax, (community_number, region_names) in zip(axes, enumerate(community_clustering_with_names)):\n",
    "\n",
    "    region_indices = [regions_dict[name] for name in region_names if \"Unknown\" not in name]\n",
    "    region_mask = create_region_mask(aal_atlas, region_indices)\n",
    "    plotting.plot_roi(region_mask, title=f\"Community {community_number}\", axes=ax, display_mode='ortho', draw_cross=False,cmap=mpl.cm.viridis)\n",
    "\n",
    "# Hide any unused subplots\n",
    "for ax in axes[len(community_clustering_with_names):]:\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81\n",
      "Extracted average scores (sample): [0.52794606 0.52793888 0.52784544 0.52782519 0.52780602 0.52779076\n",
      " 0.52775894 0.52772788 0.52769938 0.52767946 0.5276758  0.52770117\n",
      " 0.52767917 0.52765625 0.52763545 0.52761528 0.52758638 0.52755976\n",
      " 0.52758061 0.52760026]\n",
      "Total number of ROIs: 120\n",
      "Top 25% ROI indices: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 29, 30]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "\n",
    "# Define the variables\n",
    "date_time = '2024-06-03_21-50-02 bl'\n",
    "z_scoring = 'Z_scoring'\n",
    "percentile = 10\n",
    "class_pair1 = ['CN', 'SMC']\n",
    "class_pair2 = ['MCI', 'EMCI', 'LMCI']\n",
    "# Construct class pair strings with single quotes\n",
    "class_pair1_str = ', '.join([f\"'{c}'\" for c in class_pair1])\n",
    "class_pair2_str = ', '.join([f\"'{c}'\" for c in class_pair2])\n",
    "\n",
    "# Construct the file path\n",
    "\n",
    "file_path = f\"/home/hang/GitHub/BrainGNN_Pytorch/results/{date_time}/{z_scoring}/{percentile}/[{class_pair1_str}]_vs_{class_pair2}/results/score1s.json\"\n",
    "\n",
    "# Load the JSON data from the uploaded file\n",
    "with open(file_path, 'r') as f:\n",
    "    scores = json.load(f)\n",
    "\n",
    "# Function to recursively flatten lists and filter numeric values\n",
    "def flatten_and_filter(lst):\n",
    "    flat_list = []\n",
    "    for item in lst:\n",
    "        if isinstance(item, (list, tuple)):\n",
    "            flat_list.extend(flatten_and_filter(item))\n",
    "        elif isinstance(item, (int, float)):\n",
    "            flat_list.append(item)\n",
    "    return flat_list\n",
    "\n",
    "values = []\n",
    "for key in scores.keys():\n",
    "    values.extend(flatten_and_filter(scores[key]))\n",
    "# Remove the last 60 digits from the values list\n",
    "\n",
    "# Check if the values list is empty before proceeding\n",
    "if len(values) == 0:\n",
    "    top_indices_list = \"No valid numeric data found.\"\n",
    "else:\n",
    "    # Ensure that the number of values is a multiple of 120\n",
    "    if len(values) % 120 != 0:\n",
    "        raise ValueError(\"The number of values is not a multiple of 120 after removing the last 60 digits. Please check the input data.\")\n",
    "\n",
    "    num_subjects = len(values) // 120\n",
    "    subject_scores = []\n",
    "\n",
    "    # Iterate over the values in chunks of 120\n",
    "    for i in range(num_subjects):\n",
    "        chunk = values[i*120:(i+1)*120]\n",
    "        subject_scores.append(chunk)\n",
    "    print(len(subject_scores))\n",
    "    # Convert the list of subject scores to a NumPy array\n",
    "    subject_scores = np.array(subject_scores)\n",
    "    \n",
    "    # Calculate the average score for each ROI across all subjects\n",
    "    average_scores = np.mean(subject_scores, axis=0)\n",
    "    \n",
    "    # Determine the threshold for the top 25% of the average scores\n",
    "    threshold = np.percentile(average_scores, 75)\n",
    "    \n",
    "    # Find the indices of the ROIs that have average scores above the threshold\n",
    "    top_indices = np.where(average_scores >= threshold)[0]\n",
    "    \n",
    "    # Convert indices to list for easy readability\n",
    "    top_indices_list = top_indices.tolist()\n",
    "\n",
    "print(f\"Extracted average scores (sample): {average_scores[:20]}\")  # Print first 20 average scores for inspection\n",
    "print(f\"Total number of ROIs: {len(average_scores)}\")\n",
    "print(f\"Top 25% ROI indices: {top_indices_list}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 43  38  50  84 113  71  68  48  82  72 110  97  44 116  95  87 108  86\n",
      " 115  61 113  68  80  42  79  52  20  97  93  59  60  65 113  44 111 109\n",
      "  22  66  80  96 107  92  99 107  83  89  77 109 102  73  96  90]\n",
      "[23 52 16 46 20 24 30 53 49  7 34  5  3 24 23 29  8 56 59 17 40 29  9 18\n",
      " 22 34 49 46 52 57 41 48 47 35 38 55 53 44 27 42 43 28 51 46 41 29 54 34\n",
      " 57 31 35 36]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 52 is out of bounds for axis 0 with size 52",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[103], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m perm2s \u001b[38;5;241m=\u001b[39m perm2s_1\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# print(perm2s.shape)\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Convert the indices to match the original graph nodes\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m final_node_indices \u001b[38;5;241m=\u001b[39m \u001b[43mperm1s\u001b[49m\u001b[43m[\u001b[49m\u001b[43mperm2s\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(final_node_indices))\n\u001b[1;32m     19\u001b[0m chunk_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m30\u001b[39m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 52 is out of bounds for axis 0 with size 52"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "#perm1s_0 = np.load('results/2024-06-10_09-23-12/perturbation/20/CN_vs_MCI/results/perm1_class_0.npy') \n",
    "perm1s_1 = np.load('results/2024-06-10_11-03-36/perturbation/5/CN_vs_MCI/results/perm1_class_1.npy')\n",
    "print(perm1s_1)\n",
    "#perm1s = np.concatenate([perm1s_0,perm1s_1])\n",
    "#print(perm1s.shape)\n",
    "#perm2s_0 = np.load('results/2024-06-10_09-23-12/perturbation/20/CN_vs_MCI/results/perm2_class_0.npy') \n",
    "perm2s_1 = np.load('results/2024-06-10_11-03-36/perturbation/5/CN_vs_MCI/results/perm2_class_1.npy')\n",
    "#perm2s = np.concatenate([perm2s_0,perm2s_1])\n",
    "print(perm2s_1)\n",
    "# Flatten the indices if they were saved as lists of lists\n",
    "perm1s = perm1s_1.flatten()\n",
    "perm2s = perm2s_1.flatten()\n",
    "# print(perm2s.shape)\n",
    "# Convert the indices to match the original graph nodes\n",
    "final_node_indices = perm1s[perm2s]\n",
    "print(len(final_node_indices))\n",
    "chunk_size = 30\n",
    "subject_perms = []\n",
    "print(len(final_node_indices)/30)\n",
    "for i in range(int(len(final_node_indices)/chunk_size)):\n",
    "\n",
    "    subject_perms.append(final_node_indices[i*chunk_size:(i+1)*chunk_size]%120)\n",
    "subject_perms\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'perm1_class_0.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[112], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m     value_to_region[third_col_value] \u001b[38;5;241m=\u001b[39m region_name\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Load the saved perm1, perm2, and node indices for each class\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m perm1s_0 \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mperm1_class_0.npy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \n\u001b[1;32m     13\u001b[0m perm1s_1 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mperm1_class_1.npy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     14\u001b[0m perm1s \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate([perm1s_0, perm1s_1])\n",
      "File \u001b[0;32m~/anaconda3/conda3/envs/BrainGNN/lib/python3.9/site-packages/numpy/lib/npyio.py:427\u001b[0m, in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[1;32m    425\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    426\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 427\u001b[0m     fid \u001b[38;5;241m=\u001b[39m stack\u001b[38;5;241m.\u001b[39menter_context(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos_fspath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    428\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    430\u001b[0m \u001b[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'perm1_class_0.npy'"
     ]
    }
   ],
   "source": [
    "regions_dict = {}\n",
    "value_to_region = {}\n",
    "for line in regions_data.strip().split('\\n'):\n",
    "    parts = line.split()\n",
    "    region_name = parts[0]\n",
    "    second_col_value = int(parts[1])\n",
    "    third_col_value = int(parts[2])\n",
    "    regions_dict[second_col_value] = region_name\n",
    "    value_to_region[third_col_value] = region_name\n",
    "\n",
    "# Load the saved perm1, perm2, and node indices for each class\n",
    "perm1s_0 = np.load('results/2024-06-10_11-21-34/perturbation/20/MCI_vs_Dementia/results/perm1_class_0.npy') \n",
    "perm1s_1 = np.load('results/2024-06-10_11-21-34/perturbation/20/MCI_vs_Dementia/results/perm1_class_1.npy')\n",
    "perm1s = np.concatenate([perm1s_0, perm1s_1])\n",
    "print(perm1s.shape)\n",
    "\n",
    "perm2s_0 = np.load('results/2024-06-10_11-21-34/perturbation/20/MCI_vs_Dementia/results/perm2_class_1.npy') \n",
    "perm2s_1 = np.load('results/2024-06-10_11-21-34/perturbation/20/MCI_vs_Dementia/results/perm2_class_1.npy')\n",
    "perm2s = np.concatenate([perm2s_0, perm2s_1])\n",
    "print(perm2s.shape)\n",
    "\n",
    "node_indices_0 = np.load('node_indices_class_0.npy')\n",
    "node_indices_1 = np.load('node_indices_class_1.npy')\n",
    "node_indices = np.concatenate([node_indices_0, node_indices_1])\n",
    "print(node_indices.shape)\n",
    "\n",
    "# Flatten the indices if they were saved as lists of lists\n",
    "perm1s = perm1s.flatten()\n",
    "perm2s = perm2s.flatten()\n",
    "print(perm1s.shape)\n",
    "print(perm2s.shape)\n",
    "\n",
    "# Convert the indices to match the original graph nodes\n",
    "final_node_indices = perm1s[perm2s]\n",
    "print(len(final_node_indices))\n",
    "\n",
    "# Split the indices by subject (assuming each subject has 120 nodes originally)\n",
    "chunk_size = 120  # Number of nodes per subject before pooling\n",
    "subject_perms = []\n",
    "\n",
    "print(len(final_node_indices) / chunk_size)\n",
    "for i in range(int(len(final_node_indices) / chunk_size)):\n",
    "    subject_perms.append(final_node_indices[i * chunk_size:(i + 1) * chunk_size])\n",
    "\n",
    "# Function to visualize the indices for a given class\n",
    "def visualize_class_indices(subject_perms, class_label, num_subjects):\n",
    "    for i in range(num_subjects):\n",
    "        perms = subject_perms[i]\n",
    "        final_regions = [value_to_region.get(idx, 'Unknown') for idx in perms]\n",
    "\n",
    "        # Extract coordinates of the retained regions\n",
    "        atlas_img = image.index_img(aal_atlas, [idx for idx in perms])\n",
    "\n",
    "        # Plot the regions on the brain atlas\n",
    "        display = plotting.plot_anat()\n",
    "        plotting.plot_roi(atlas_img, display_mode='ortho', draw_cross=True, title=f'Retained Nodes for Class {class_label} Subject {i} after Pooling')\n",
    "        plt.show()\n",
    "\n",
    "# Visualize for a specific class (e.g., class 0)\n",
    "num_subjects_class_0 = len(node_indices_0) // chunk_size\n",
    "visualize_class_indices(subject_perms[:num_subjects_class_0], class_label=0, num_subjects=num_subjects_class_0)\n",
    "\n",
    "# Visualize for another class (e.g., class 1)\n",
    "num_subjects_class_1 = len(node_indices_1) // chunk_size\n",
    "visualize_class_indices(subject_perms[num_subjects_class_0:], class_label=1, num_subjects=num_subjects_class_1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
