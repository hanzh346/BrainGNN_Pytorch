{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "hang_FDG = pd.read_csv('/media/hang/EXTERNAL_US/Data/1_HANG_FDG_PET/ADNI_Second_organized/1_HANG_FDG_PET_1_30_2024.csv')\n",
    "hang_FDG[hang_FDG['Description'].str.contains('6mm')].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ADNI_merge = pd.read_csv('/media/hang/EXTERNAL_US/Data/1_HANG_FDG_PET/ADNIMERGE_30Jan2024.csv')\n",
    "ADNI_merge[ADNI_merge['AV45_bl'] < 0.79]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hang_FDG_all = pd.read_csv('/media/hang/EXTERNAL_US/Data/1_HANG_FDG_PET/longitudinal_AD_MCI_CN/4_TRIAL_HANG_5_10_2024.csv')\n",
    "hang_FDG_all[hang_FDG_all['Description'].str.contains('6mm')].shape\n",
    "\n",
    "hang_FDG_ab_pos = pd.read_csv('/media/hang/EXTERNAL_US/Data/1_HANG_FDG_PET/longitudinal_AD_MCI_CN/3_HANG_LONG_5_03_2024.csv')\n",
    "\n",
    "ab_pos_cases = hang_FDG_ab_pos[hang_FDG_ab_pos['Description'].str.contains('6mm')]['Subject'].drop_duplicates()\n",
    "print(len(set(ab_pos_cases)))\n",
    "\n",
    "hang_FDG_CN_ab_neg = pd.read_csv('/media/hang/EXTERNAL_US/Data/1_HANG_FDG_PET/longitudinal_CN/4_ALL_FDG_3_27_2024.csv')\n",
    "ab_neg_cases = hang_FDG_CN_ab_neg[hang_FDG_CN_ab_neg['Description'].str.contains('6mm')]['Subject'].drop_duplicates()\n",
    "print(len(set(ab_neg_cases)))\n",
    "\n",
    "\n",
    "union_set_longitudinal = set(ab_pos_cases).union(set(ab_neg_cases))\n",
    "union_set_longitudinal\n",
    "intersect_set_longitudinal = set(ab_pos_cases).intersection(set(ab_neg_cases))\n",
    "intersect_set_longitudinal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hang_FDG_ab_pos_MRI = hang_FDG_ab_pos[hang_FDG_ab_pos['Description'].str.contains('MT1')]\n",
    "print(pd.DataFrame(hang_FDG_ab_pos_MRI).sort_values(by='Subject').drop_duplicates(subset='Subject',keep='first'))\n",
    "hang_FDG_ab_pos_PET = hang_FDG_ab_pos[hang_FDG_ab_pos['Description'].str.contains('6mm')]\n",
    "#print(pd.DataFrame(hang_FDG_ab_pos_PET).sort_values(by='Subject').drop_duplicates(subset='Subject',keep='first'))\n",
    "\n",
    "hang_FDG_ab_pos_PET['Acq Date'] = pd.to_datetime(hang_FDG_ab_pos_PET['Acq Date'])\n",
    "hang_FDG_ab_pos_MRI['Acq Date'] = pd.to_datetime(hang_FDG_ab_pos_MRI['Acq Date'])\n",
    "print('overlaped_ab+ PET and MRI',len(set(hang_FDG_ab_pos_PET['Subject']) & set(hang_FDG_ab_pos_MRI['Subject'])))\n",
    "\n",
    "hang_FDG_ab_neg_MRI = hang_FDG_CN_ab_neg[hang_FDG_CN_ab_neg['Description'].str.contains('MT1')]\n",
    "hang_FDG_ab_neg_PET = hang_FDG_CN_ab_neg[hang_FDG_CN_ab_neg['Description'].str.contains('6mm')]\n",
    "\n",
    "hang_FDG_ab_neg_PET['Acq Date'] = pd.to_datetime(hang_FDG_ab_neg_PET['Acq Date'])\n",
    "hang_FDG_ab_neg_MRI['Acq Date'] = pd.to_datetime(hang_FDG_ab_neg_MRI['Acq Date'])\n",
    "print('overlaped_ab- PET and MRI',len(set(hang_FDG_ab_neg_PET['Subject']) & set(hang_FDG_ab_neg_MRI['Subject'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Initialize an empty list to store results\n",
    "smallest_gaps = []\n",
    "\n",
    "# Process each subject's data separately\n",
    "for subject in hang_FDG_ab_pos_PET['Subject'].unique():\n",
    "    # Get all PET and MRI records for this subject\n",
    "    pet_subject = hang_FDG_ab_pos_PET[hang_FDG_ab_pos_PET['Subject'] == subject]\n",
    "    mri_subject = hang_FDG_ab_pos_MRI[hang_FDG_ab_pos_MRI['Subject'] == subject]\n",
    "   \n",
    "\n",
    "    # Initialize the minimum gap as a large value\n",
    "    min_gap = pd.Timedelta(days=9999)\n",
    "\n",
    "    # Compare each PET date with every MRI date\n",
    "    if mri_subject.empty:\n",
    "        print('No MRI subjects for {}'.format(subject))\n",
    "    for pet_date in pet_subject['Acq Date']:\n",
    "        for mri_date in mri_subject['Acq Date']:\n",
    "            # Calculate the absolute difference\n",
    "            gap = abs(pet_date - mri_date)\n",
    "            # Update the minimum gap if the current one is smaller\n",
    "            if gap < min_gap:\n",
    "                min_gap = gap\n",
    "                best_pair = (pet_date, mri_date)\n",
    "\n",
    "    # If there is a valid match, add the smallest gap information\n",
    "    if min_gap != pd.Timedelta(days=9999):\n",
    "        if min_gap > pd.Timedelta(days=365):\n",
    "            continue\n",
    "        smallest_gaps.append({\n",
    "            'Subject': subject,\n",
    "            'PET_Date': best_pair[0],\n",
    "            'MRI_Date': best_pair[1],\n",
    "            'Smallest_Gap': min_gap\n",
    "        })\n",
    "\n",
    "# Convert the results to a DataFrame\n",
    "smallest_gaps_df = pd.DataFrame(smallest_gaps).drop_duplicates()\n",
    "#smallest_gaps_df.to_csv('/media/hang/EXTERNAL_US/Data/1_HANG_FDG_PET/longitudinal_CN/negative_longitudinal_PET_MRI_pair.csv')\n",
    "smallest_gaps_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>ImageDataID</th>\n",
       "      <th>Subject</th>\n",
       "      <th>Group</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>Visit</th>\n",
       "      <th>Modality</th>\n",
       "      <th>Description</th>\n",
       "      <th>Type</th>\n",
       "      <th>...</th>\n",
       "      <th>RIGHT_VENTRALDC_SUVR</th>\n",
       "      <th>RIGHT_VENTRALDC_VOLUME</th>\n",
       "      <th>RIGHT_VESSEL_SUVR</th>\n",
       "      <th>RIGHT_VESSEL_VOLUME</th>\n",
       "      <th>WM_HYPOINTENSITIES_SUVR</th>\n",
       "      <th>WM_HYPOINTENSITIES_VOLUME</th>\n",
       "      <th>TOTAL_INTRACRANIAL_VOLUME</th>\n",
       "      <th>update_stamp_y</th>\n",
       "      <th>binary_labels_cross_section</th>\n",
       "      <th>binary_labels_longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>I1593083</td>\n",
       "      <td>002_S_4171</td>\n",
       "      <td>LMCI</td>\n",
       "      <td>M</td>\n",
       "      <td>69</td>\n",
       "      <td>v03</td>\n",
       "      <td>PET</td>\n",
       "      <td>Coreg, Avg, Std Img and Vox Siz, Uniform 6mm Res</td>\n",
       "      <td>Processed</td>\n",
       "      <td>...</td>\n",
       "      <td>1.6535</td>\n",
       "      <td>4032</td>\n",
       "      <td>1.6759</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.8061</td>\n",
       "      <td>783</td>\n",
       "      <td>1.576281e+06</td>\n",
       "      <td>2021-01-19 10:55:04</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>I1593114</td>\n",
       "      <td>002_S_4213</td>\n",
       "      <td>CN</td>\n",
       "      <td>F</td>\n",
       "      <td>78</td>\n",
       "      <td>v03</td>\n",
       "      <td>PET</td>\n",
       "      <td>Coreg, Avg, Std Img and Vox Siz, Uniform 6mm Res</td>\n",
       "      <td>Processed</td>\n",
       "      <td>...</td>\n",
       "      <td>1.3132</td>\n",
       "      <td>3689</td>\n",
       "      <td>1.1143</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1.4608</td>\n",
       "      <td>1476</td>\n",
       "      <td>1.508126e+06</td>\n",
       "      <td>2021-01-19 10:55:04</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>I1593109</td>\n",
       "      <td>002_S_4225</td>\n",
       "      <td>CN</td>\n",
       "      <td>M</td>\n",
       "      <td>70</td>\n",
       "      <td>v03</td>\n",
       "      <td>PET</td>\n",
       "      <td>Coreg, Avg, Std Img and Vox Siz, Uniform 6mm Res</td>\n",
       "      <td>Processed</td>\n",
       "      <td>...</td>\n",
       "      <td>1.6963</td>\n",
       "      <td>4545</td>\n",
       "      <td>1.6395</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.7398</td>\n",
       "      <td>2185</td>\n",
       "      <td>1.908584e+06</td>\n",
       "      <td>2021-01-19 10:55:04</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>I1593115</td>\n",
       "      <td>002_S_4262</td>\n",
       "      <td>CN</td>\n",
       "      <td>F</td>\n",
       "      <td>73</td>\n",
       "      <td>v03</td>\n",
       "      <td>PET</td>\n",
       "      <td>Coreg, Avg, Std Img and Vox Siz, Uniform 6mm Res</td>\n",
       "      <td>Processed</td>\n",
       "      <td>...</td>\n",
       "      <td>1.6008</td>\n",
       "      <td>3464</td>\n",
       "      <td>1.6747</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.6857</td>\n",
       "      <td>6717</td>\n",
       "      <td>1.552242e+06</td>\n",
       "      <td>2021-01-19 10:55:04</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>I1593110</td>\n",
       "      <td>002_S_4270</td>\n",
       "      <td>CN</td>\n",
       "      <td>F</td>\n",
       "      <td>75</td>\n",
       "      <td>v03</td>\n",
       "      <td>PET</td>\n",
       "      <td>Coreg, Avg, Std Img and Vox Siz, Uniform 6mm Res</td>\n",
       "      <td>Processed</td>\n",
       "      <td>...</td>\n",
       "      <td>1.7111</td>\n",
       "      <td>3514</td>\n",
       "      <td>1.5387</td>\n",
       "      <td>34.0</td>\n",
       "      <td>1.7918</td>\n",
       "      <td>2820</td>\n",
       "      <td>1.495694e+06</td>\n",
       "      <td>2021-01-19 10:55:04</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1337</th>\n",
       "      <td>1347</td>\n",
       "      <td>I1603417</td>\n",
       "      <td>941_S_4377</td>\n",
       "      <td>LMCI</td>\n",
       "      <td>F</td>\n",
       "      <td>70</td>\n",
       "      <td>v03</td>\n",
       "      <td>PET</td>\n",
       "      <td>Coreg, Avg, Std Img and Vox Siz, Uniform 6mm Res</td>\n",
       "      <td>Processed</td>\n",
       "      <td>...</td>\n",
       "      <td>1.3244</td>\n",
       "      <td>3307</td>\n",
       "      <td>1.0539</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.5795</td>\n",
       "      <td>972</td>\n",
       "      <td>1.311141e+06</td>\n",
       "      <td>2021-01-19 10:55:04</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1339</th>\n",
       "      <td>1349</td>\n",
       "      <td>I1603375</td>\n",
       "      <td>941_S_4420</td>\n",
       "      <td>EMCI</td>\n",
       "      <td>M</td>\n",
       "      <td>82</td>\n",
       "      <td>v03</td>\n",
       "      <td>PET</td>\n",
       "      <td>Coreg, Avg, Std Img and Vox Siz, Uniform 6mm Res</td>\n",
       "      <td>Processed</td>\n",
       "      <td>...</td>\n",
       "      <td>1.2943</td>\n",
       "      <td>3942</td>\n",
       "      <td>1.3754</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.3988</td>\n",
       "      <td>3596</td>\n",
       "      <td>1.664610e+06</td>\n",
       "      <td>2021-01-19 10:55:04</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1341</th>\n",
       "      <td>1351</td>\n",
       "      <td>I1603380</td>\n",
       "      <td>941_S_4764</td>\n",
       "      <td>EMCI</td>\n",
       "      <td>F</td>\n",
       "      <td>83</td>\n",
       "      <td>v03</td>\n",
       "      <td>PET</td>\n",
       "      <td>Coreg, Avg, Std Img and Vox Siz, Uniform 6mm Res</td>\n",
       "      <td>Processed</td>\n",
       "      <td>...</td>\n",
       "      <td>1.2174</td>\n",
       "      <td>3725</td>\n",
       "      <td>1.0669</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1.2939</td>\n",
       "      <td>8413</td>\n",
       "      <td>1.488525e+06</td>\n",
       "      <td>2021-01-19 10:55:05</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1343</th>\n",
       "      <td>1353</td>\n",
       "      <td>I1603596</td>\n",
       "      <td>941_S_5124</td>\n",
       "      <td>SMC</td>\n",
       "      <td>F</td>\n",
       "      <td>77</td>\n",
       "      <td>v03</td>\n",
       "      <td>PET</td>\n",
       "      <td>Coreg, Avg, Std Img and Vox Siz, Uniform 6mm Res</td>\n",
       "      <td>Processed</td>\n",
       "      <td>...</td>\n",
       "      <td>1.6743</td>\n",
       "      <td>2620</td>\n",
       "      <td>1.4851</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.2004</td>\n",
       "      <td>6689</td>\n",
       "      <td>1.494192e+06</td>\n",
       "      <td>2021-01-19 10:55:05</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1345</th>\n",
       "      <td>1355</td>\n",
       "      <td>I1603407</td>\n",
       "      <td>941_S_5193</td>\n",
       "      <td>SMC</td>\n",
       "      <td>F</td>\n",
       "      <td>73</td>\n",
       "      <td>v03</td>\n",
       "      <td>PET</td>\n",
       "      <td>Coreg, Avg, Std Img and Vox Siz, Uniform 6mm Res</td>\n",
       "      <td>Processed</td>\n",
       "      <td>...</td>\n",
       "      <td>1.2684</td>\n",
       "      <td>3865</td>\n",
       "      <td>0.8988</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.3668</td>\n",
       "      <td>2491</td>\n",
       "      <td>1.468981e+06</td>\n",
       "      <td>2021-01-19 10:55:06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>531 rows Ã— 377 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0 ImageDataID     Subject Group Sex  Age Visit Modality  \\\n",
       "1              1    I1593083  002_S_4171  LMCI   M   69   v03      PET   \n",
       "3              3    I1593114  002_S_4213    CN   F   78   v03      PET   \n",
       "7              7    I1593109  002_S_4225    CN   M   70   v03      PET   \n",
       "15            15    I1593115  002_S_4262    CN   F   73   v03      PET   \n",
       "17            17    I1593110  002_S_4270    CN   F   75   v03      PET   \n",
       "...          ...         ...         ...   ...  ..  ...   ...      ...   \n",
       "1337        1347    I1603417  941_S_4377  LMCI   F   70   v03      PET   \n",
       "1339        1349    I1603375  941_S_4420  EMCI   M   82   v03      PET   \n",
       "1341        1351    I1603380  941_S_4764  EMCI   F   83   v03      PET   \n",
       "1343        1353    I1603596  941_S_5124   SMC   F   77   v03      PET   \n",
       "1345        1355    I1603407  941_S_5193   SMC   F   73   v03      PET   \n",
       "\n",
       "                                           Description       Type  ...  \\\n",
       "1     Coreg, Avg, Std Img and Vox Siz, Uniform 6mm Res  Processed  ...   \n",
       "3     Coreg, Avg, Std Img and Vox Siz, Uniform 6mm Res  Processed  ...   \n",
       "7     Coreg, Avg, Std Img and Vox Siz, Uniform 6mm Res  Processed  ...   \n",
       "15    Coreg, Avg, Std Img and Vox Siz, Uniform 6mm Res  Processed  ...   \n",
       "17    Coreg, Avg, Std Img and Vox Siz, Uniform 6mm Res  Processed  ...   \n",
       "...                                                ...        ...  ...   \n",
       "1337  Coreg, Avg, Std Img and Vox Siz, Uniform 6mm Res  Processed  ...   \n",
       "1339  Coreg, Avg, Std Img and Vox Siz, Uniform 6mm Res  Processed  ...   \n",
       "1341  Coreg, Avg, Std Img and Vox Siz, Uniform 6mm Res  Processed  ...   \n",
       "1343  Coreg, Avg, Std Img and Vox Siz, Uniform 6mm Res  Processed  ...   \n",
       "1345  Coreg, Avg, Std Img and Vox Siz, Uniform 6mm Res  Processed  ...   \n",
       "\n",
       "     RIGHT_VENTRALDC_SUVR RIGHT_VENTRALDC_VOLUME RIGHT_VESSEL_SUVR  \\\n",
       "1                  1.6535                   4032            1.6759   \n",
       "3                  1.3132                   3689            1.1143   \n",
       "7                  1.6963                   4545            1.6395   \n",
       "15                 1.6008                   3464            1.6747   \n",
       "17                 1.7111                   3514            1.5387   \n",
       "...                   ...                    ...               ...   \n",
       "1337               1.3244                   3307            1.0539   \n",
       "1339               1.2943                   3942            1.3754   \n",
       "1341               1.2174                   3725            1.0669   \n",
       "1343               1.6743                   2620            1.4851   \n",
       "1345               1.2684                   3865            0.8988   \n",
       "\n",
       "      RIGHT_VESSEL_VOLUME WM_HYPOINTENSITIES_SUVR WM_HYPOINTENSITIES_VOLUME  \\\n",
       "1                    10.0                  1.8061                       783   \n",
       "3                    12.0                  1.4608                      1476   \n",
       "7                     3.0                  1.7398                      2185   \n",
       "15                    9.0                  1.6857                      6717   \n",
       "17                   34.0                  1.7918                      2820   \n",
       "...                   ...                     ...                       ...   \n",
       "1337                  5.0                  1.5795                       972   \n",
       "1339                  6.0                  1.3988                      3596   \n",
       "1341                 20.0                  1.2939                      8413   \n",
       "1343                  3.0                  1.2004                      6689   \n",
       "1345                  1.0                  1.3668                      2491   \n",
       "\n",
       "     TOTAL_INTRACRANIAL_VOLUME      update_stamp_y  \\\n",
       "1                 1.576281e+06 2021-01-19 10:55:04   \n",
       "3                 1.508126e+06 2021-01-19 10:55:04   \n",
       "7                 1.908584e+06 2021-01-19 10:55:04   \n",
       "15                1.552242e+06 2021-01-19 10:55:04   \n",
       "17                1.495694e+06 2021-01-19 10:55:04   \n",
       "...                        ...                 ...   \n",
       "1337              1.311141e+06 2021-01-19 10:55:04   \n",
       "1339              1.664610e+06 2021-01-19 10:55:04   \n",
       "1341              1.488525e+06 2021-01-19 10:55:05   \n",
       "1343              1.494192e+06 2021-01-19 10:55:05   \n",
       "1345              1.468981e+06 2021-01-19 10:55:06   \n",
       "\n",
       "     binary_labels_cross_section binary_labels_longitude  \n",
       "1                            2.0                     2.0  \n",
       "3                            0.0                     0.0  \n",
       "7                            1.0                     1.0  \n",
       "15                           1.0                     1.0  \n",
       "17                           1.0                     1.0  \n",
       "...                          ...                     ...  \n",
       "1337                         2.0                     2.0  \n",
       "1339                         2.0                     2.0  \n",
       "1341                         2.0                     2.0  \n",
       "1343                         1.0                     0.0  \n",
       "1345                         0.0                     0.0  \n",
       "\n",
       "[531 rows x 377 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "\n",
    "class_pairs = [['CN', 'SMC'], ['CN', 'SMC'],['EMCI', 'LMCI'],'AD']\n",
    "UCBERKLEY_PET = pd.read_excel('/home/hang/GU/Project/AD_classification_synthesis/data/ADNI/UCBERKELEYAV45_01_14_21.xlsx')\n",
    "dataTable = pd.read_csv('/home/hang/GitHub/BrainGNN_Pytorch/data/filtered_selectedDataUnique_merged_ADNI.csv')\n",
    "\n",
    "# Filter UCBERKLEY_PET for baseline rows if there is a specific column indicating baseline (assuming 'VISCODE' column here)\n",
    "UCBERKLEY_PET_bl = UCBERKLEY_PET[UCBERKLEY_PET['VISCODE2'] == 'bl']\n",
    "\n",
    "# Perform an inner join on the Subject column\n",
    "merged_data = pd.merge(dataTable, UCBERKLEY_PET_bl, left_on='RID', right_on='RID', how='inner').drop_duplicates(subset='Subject',keep='last')\n",
    "\n",
    "merged_data['binary_labels_cross_section'] = merged_data.apply(lambda row:\n",
    "            0 if row['DX_bl'] in class_pairs[0] and row['SUMMARYSUVR_WHOLECEREBNORM'] < 1.11 else\n",
    "            1 if row['DX_bl'] in class_pairs[1] and row['SUMMARYSUVR_WHOLECEREBNORM'] >= 1.11 else\n",
    "            2 if row['DX_bl'] in class_pairs[2] and row['SUMMARYSUVR_WHOLECEREBNORM'] >= 1.11 else\n",
    "            3 if row['DX_bl'] in class_pairs[3] and row['SUMMARYSUVR_WHOLECEREBNORM'] >= 1.11 else np.nan, axis=1)\n",
    "\n",
    "merged_data['binary_labels_longitude'] = merged_data.apply(lambda row:\n",
    "            0 if row['DX_bl'] in class_pairs[0] and row['SUMMARYSUVR_COMPOSITE_REFNORM'] < 0.79 else\n",
    "            1 if row['DX_bl'] in class_pairs[1] and row['SUMMARYSUVR_COMPOSITE_REFNORM'] >= 0.79 else\n",
    "            2 if row['DX_bl'] in class_pairs[2] and row['SUMMARYSUVR_COMPOSITE_REFNORM'] >= 0.79 else\n",
    "            3 if row['DX_bl'] in class_pairs[3] and row['SUMMARYSUVR_COMPOSITE_REFNORM'] >= 0.79 else np.nan, axis=1)\n",
    "\n",
    "filtered_data = merged_data.dropna(subset=['binary_labels_cross_section'])\n",
    "# filtered_data = filtered_data.dropna(subset=['binary_labels_longitude'])\n",
    "filtered_data= filtered_data.drop_duplicates(subset='Subject', keep='last')\n",
    "merged_data.to_csv('merged_filtered_selected_data_ADNI_merge_with_UCBERKELEYAV45_01_14_21.csv')\n",
    "filtered_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import timeit\n",
    "import networkx as nx\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import networkx as nx\n",
    "from scipy.io import loadmat\n",
    "from torch_geometric.data import Data,  Batch\n",
    "from torch_geometric.utils import  remove_self_loops\n",
    "import multiprocessing\n",
    "from torch_sparse import coalesce\n",
    "from functools import partial\n",
    "def extract_features(file_path, field_name='scaledMahalDistMatrix'):\n",
    "    \"\"\"\n",
    "    Placeholder function for extracting features from a .mat file.\n",
    "    Implement according to your specific requirements.\n",
    "    \"\"\"\n",
    "    mat = loadmat(file_path)\n",
    "    return mat[field_name]\n",
    "\n",
    "def apply_threshold(matrix, percentile):\n",
    "    # Flatten the matrix and sort it\n",
    "    sorted_matrix = np.sort(matrix)\n",
    "    # Calculate the index for the desired percentile\n",
    "    threshold_index = round(len(sorted_matrix) * (100 - percentile) / 100)  # Adjust for zero indexing\n",
    "\n",
    "    # Get the threshold value using the sorted matrix\n",
    "    threshold_value = sorted_matrix[threshold_index]\n",
    "    # Apply thresholding\n",
    "    thresholded_matrix = np.where(matrix > threshold_value, matrix, 0)\n",
    "    return thresholded_matrix\n",
    "\n",
    "\n",
    "def process_subject(subject_id, binary_label, mat_files_dir, percentile, connectome):\n",
    "    \"\"\"\n",
    "    Process a single subject's graph data for classification, including binary label.\n",
    "    \"\"\"\n",
    "    mat_file_path = os.path.join(mat_files_dir, f\"{subject_id}_{connectome}.mat\")\n",
    "    par_corr_file_path = os.path.join(mat_files_dir, f\"{subject_id}_partial_correlation_KDE.mat\")\n",
    "\n",
    "    # Check if files exist\n",
    "    if not os.path.exists(mat_file_path) or not os.path.exists(par_corr_file_path):\n",
    "        print(f\"Files for subject {subject_id} are missing. Paths: {mat_file_path}, {par_corr_file_path}\")\n",
    "        return None\n",
    "    \n",
    "    # Determine the field name based on connectome type\n",
    "    field_name = {\n",
    "        'ScaledMahalanobisDistanceMatrix': 'scaledMahalDistMatrix',\n",
    "        'Z_scoring': 'zScoreConnectivityMatrix',\n",
    "        'K_correlation': 'correlationMatrix',\n",
    "        'JSdivMatrix': 'K_JS_Divergence'\n",
    "    }.get(connectome, None)\n",
    "    \n",
    "\n",
    "    print(f\"Processing subject {subject_id} with connectome: {connectome}\")\n",
    "\n",
    "    # Extract features\n",
    "    node_features = apply_threshold(extract_features(mat_file_path, field_name=field_name), percentile=percentile)\n",
    "    edge_features = apply_threshold(extract_features(par_corr_file_path, field_name='partial_corr'), percentile=percentile)\n",
    "\n",
    "\n",
    "    # Construct graph\n",
    "    num_nodes = node_features.shape[0]\n",
    "    G = nx.from_numpy_array(edge_features)\n",
    "    A = nx.to_scipy_sparse_array(G, format='coo')\n",
    "    adj = A.tocoo()\n",
    "\n",
    "    # Create edge attributes\n",
    "    edge_att = np.zeros(len(adj.row))\n",
    "    for i in range(len(adj.row)):\n",
    "        edge_att[i] = node_features[adj.row[i], adj.col[i]]\n",
    "\n",
    "    edge_index = np.stack([A.row, A.col])\n",
    "    edge_index, edge_att = remove_self_loops(torch.from_numpy(edge_index), torch.from_numpy(edge_att))\n",
    "    edge_index = edge_index.long()\n",
    "    edge_index, edge_att = coalesce(edge_index, edge_att, num_nodes, num_nodes)\n",
    "    pos = torch.eye(num_nodes)  # Using an identity matrix for positional data\n",
    "\n",
    "    return Data(x=torch.tensor(node_features, dtype=torch.float),\n",
    "                edge_index=edge_index,\n",
    "                edge_attr=edge_att,\n",
    "                y=torch.tensor([binary_label], dtype=torch.long),\n",
    "                pos=pos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# List to store processed data\n",
    "all_data = []\n",
    "\n",
    "# Directory containing the .mat files\n",
    "mat_files_dir = \"/media/hang/EXTERNAL_US/Data/1_HANG_FDG_PET/ADNI_Second_organized/KDE_Results_corrected_by_age_sec_education/\"\n",
    "\n",
    "# Path to the raw data\n",
    "data_list_included = glob.glob('/media/hang/EXTERNAL_US/Data/1_HANG_FDG_PET/ADNI_Second_organized/KDE_Results/raw/*meanSUVR.mat')\n",
    "\n",
    "# Percentile parameter\n",
    "percentile = 10\n",
    "\n",
    "# Extract subject IDs from file names\n",
    "subject_ids_included = [filename.split('/')[-1].split('_meanSUVR')[0] for filename in data_list_included]\n",
    "\n",
    "# Assuming ADNI_merge and UCBERKLEY_PET are already loaded DataFrames\n",
    "# Example class_pairs definition\n",
    "class_pairs = [[\"CN\", \"SMC\"], ['MCI', 'EMCI', 'LMCI'], [\"Dementia\"]]\n",
    "connectome = 'Z_scoring'\n",
    "\n",
    "for id in subject_ids_included:\n",
    "    # Get the diagnosis values sorted by exam date for the subject\n",
    "    diagnosis_values = ADNI_merge[ADNI_merge['PTID'] == id].sort_values('EXAMDATE')['DX'].dropna().values\n",
    "    \n",
    "    # Check if there is more than one unique diagnosis\n",
    "    if len(set(diagnosis_values)) > 1:\n",
    "        continue\n",
    "\n",
    "    # Extract RID from the subject ID\n",
    "    rid = id.split('_')[-1]\n",
    "    print(f\"Extracted RID: {rid}\")\n",
    "\n",
    "    # Check the SUVR values for the extracted RID\n",
    "    suvr_values = UCBERKLEY_PET[UCBERKLEY_PET['RID'].astype(str) == rid]['SUMMARYSUVR_COMPOSITE_REFNORM']\n",
    "    \n",
    "    # Skip if SUVR values are empty or have mixed values for threshold\n",
    "    if suvr_values.empty or len(set(suvr_values > 0.79)) > 1:\n",
    "        continue\n",
    "\n",
    "    # Skip if there are fewer than 2 diagnosis values or SUVR values\n",
    "    if len(diagnosis_values) < 2 or len(suvr_values) < 2:\n",
    "        continue\n",
    "\n",
    "    # Determine the diagnosis and amyloid-beta status\n",
    "    diagnosis = diagnosis_values[0]\n",
    "    ab = (suvr_values > 0.79).astype(int).values[0]\n",
    "\n",
    "    # Set binary_label based on conditions\n",
    "    binary_label = np.nan\n",
    "    if diagnosis in class_pairs[0]:\n",
    "        binary_label = 0 if ab == 0 else 1\n",
    "    elif diagnosis in class_pairs[1] and ab == 1:\n",
    "        binary_label = 2\n",
    "    elif diagnosis in class_pairs[2] and ab == 1:\n",
    "        binary_label = 3\n",
    "    else:\n",
    "        continue\n",
    "    \n",
    "    # Process the subject data\n",
    "    data = process_subject(id, float(binary_label), mat_files_dir, percentile=percentile, connectome=connectome)\n",
    "    \n",
    "    # Append the processed data if not None\n",
    "    if data is not None:\n",
    "        all_data.append(data)\n",
    "\n",
    "\n",
    "# Print the number of processed data entries\n",
    "print(f\"Total processed data entries: {len(all_data)}\")\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of intersecting subjects: 588\n",
      "binary_labels_cross_section\n",
      "0.0    148\n",
      "2.0    144\n",
      "3.0    105\n",
      "1.0     79\n",
      "Name: count, dtype: int64\n",
      "binary_labels_longitude\n",
      "0.0    164\n",
      "2.0    136\n",
      "3.0    103\n",
      "1.0     63\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "466"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert lists to sets for intersection\n",
    "data_list_included = glob.glob('/media/hang/EXTERNAL_US/Data/1_HANG_FDG_PET/ADNI_Second_organized/KDE_Results/raw/*meanSUVR.mat')\n",
    "subject_ids_included = [filename.split('/')[-1].split('_meanSUVR')[0] for filename in data_list_included]\n",
    "set_subject_ids_included = set(subject_ids_included)\n",
    "set_filtered_subjects = set(merged_data['Subject'])\n",
    "\n",
    "\n",
    "\n",
    "# Find intersection\n",
    "intersection_subjects_bl = set_subject_ids_included.intersection(set_filtered_subjects)\n",
    "analized_data_included_bl = merged_data[merged_data['Subject'].isin(intersection_subjects_bl)]\n",
    "\n",
    "# Print intersection result\n",
    "print(f\"Number of intersecting subjects: {len(analized_data_included_bl)}\")\n",
    "\n",
    "\n",
    "print(analized_data_included_bl.value_counts(subset='binary_labels_cross_section'))\n",
    "print(analized_data_included_bl.value_counts(subset='binary_labels_longitude'))\n",
    "union_set_longitudinal = list(union_set_longitudinal)\n",
    "#filtered_data_cross_sectional_bl_longitudinal = union_set_longitudinal[union_set_longitudinal in (analized_data_included_bl['Subject'])]\n",
    "#list(set(analized_data_included_bl['Subject']) & set(union_set_longitudinal))\n",
    "## bl subjects count\n",
    "#analized_data_included_bl[analized_data_included_bl['binary_labels_cross_section']==0]['Subject']\n",
    "analized_data_included_bl\n",
    "164+136+103+63"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "subjects_not_in_filtered =  set_subject_ids_included  - set_filtered_subjects\n",
    "print(\"Subjects in set_subject_ids_included but not in set_filtered_subjects:\")\n",
    "print(len(subjects_not_in_filtered))\n",
    "subjects_not_in_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import csv\n",
    "# Filter and get the list of subjects where binary_labels is 0\n",
    "subjects_list_CN_neg = analized_data_included_bl[analized_data_included_bl['binary_labels_cross_section'] == 0]['Subject']\n",
    "subjects_list_CN_pos = analized_data_included_bl[analized_data_included_bl['binary_labels_cross_section'] == 1]['Subject']\n",
    "subjects_list_MCI_pos = analized_data_included_bl[analized_data_included_bl['binary_labels_cross_section'] == 2]['Subject']\n",
    "subjects_list_AD_pos = analized_data_included_bl[analized_data_included_bl['binary_labels_cross_section'] == 3]['Subject']\n",
    "\n",
    "# Convert the series to a list of strings\n",
    "subjects_str_list_CN_neg = [str(subject) + ',' for subject in subjects_list_CN_neg]\n",
    "subjects_str_list_CN_pos = [str(subject) + ',' for subject in subjects_list_CN_pos]\n",
    "subjects_str_list_MCI_pos = [str(subject) + ',' for subject in subjects_list_MCI_pos]\n",
    "subjects_str_list_AD_pos = [str(subject) + ',' for subject in subjects_list_AD_pos]\n",
    "\n",
    "# Function to save subjects' lists to CSV files, each subject on a new line\n",
    "def save_subjects_to_csv(filename, header, subjects_list):\n",
    "    with open(filename, 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow([header])  # Write the header\n",
    "        for subject in subjects_list:\n",
    "            writer.writerow([subject])  # Write each subject in a new line\n",
    "\n",
    "# Save each list of subjects to separate CSV files\n",
    "save_subjects_to_csv('/media/hang/EXTERNAL_US/Data/1_HANG_FDG_PET/longitudinal_AD_MCI_CN/longitudinalthres1.11_subjects_cn_neg.csv',\n",
    "                     'CN_neg', subjects_str_list_CN_neg)\n",
    "\n",
    "save_subjects_to_csv('/media/hang/EXTERNAL_US/Data/1_HANG_FDG_PET/longitudinal_AD_MCI_CN/longitudinalthres1.11_subjects_cn_pos.csv',\n",
    "                     'CN_Pos', subjects_str_list_CN_pos)\n",
    "\n",
    "save_subjects_to_csv('/media/hang/EXTERNAL_US/Data/1_HANG_FDG_PET/longitudinal_AD_MCI_CN/longitudinalthres1.11_subjects_mci_pos.csv',\n",
    "                     'MCI_Pos', subjects_str_list_MCI_pos)\n",
    "\n",
    "save_subjects_to_csv('/media/hang/EXTERNAL_US/Data/1_HANG_FDG_PET/longitudinal_AD_MCI_CN/longitudinalthres1.11_subjects_ad_pos.csv',\n",
    "                     'AD_Pos', subjects_str_list_AD_pos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_pos = list(subjects_str_list_CN_pos + subjects_str_list_MCI_pos + subjects_str_list_AD_pos)\n",
    "print(len(set(subjects_str_list_CN_neg)))\n",
    "print('ab+ overlapped cases',len(set(subject_pos) & set(ab_pos_cases)))# see the overlap of bl with longitudinal downloaded\n",
    "\n",
    "subject_neg = subjects_str_list_CN_neg\n",
    "print(set(subject_neg).intersection(set(subject_pos)))\n",
    "print('ab- overlapped cases',len(set(subject_neg) & set(ab_neg_cases)))\n",
    "print(len(set(hang_FDG_ab_pos['Subject'])))\n",
    "print(len(set(subject_pos).intersection(hang_FDG_ab_pos['Subject'])))\n",
    "print(len(set(subject_neg).intersection(hang_FDG_CN_ab_neg['Subject'])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
